\chapter{Advanced Topics}
\section{Hardware and Data Models}
\begin{sidenotebox}{The Turing Tax}
    The additional cost/overhead (performance, hardware cost, energy) of universality/general purpose computing in hardware.
    \begin{center}
        \begin{tabular}{l l p{.8\textwidth}}
                            & Example  & Description                                        \\
            General Purpose & CPU      & \textit{Jack of all trades, but a master of none.} \\
            Dedicated       & GPU, TPU & Optimised for a very specific set of operations.   \\
        \end{tabular}
    \end{center}
    The  \textit{turing tax} \& related tradeoffs of general purpose computing are discussed at length in Dr Paul Kelly's \textit{60001 - Advanced Computer Architecture} Module.
\end{sidenotebox}
\noindent
\centerline{\textit{Hardware Heterogeneity is Increasing}}
\begin{itemize}
    \item The end of moore's law the \textit{free lunch} provided decades of performance improvements by \textit{dennard scaling} is ending.
    \item Dedicated accelerators for specific applications/operations can provide increased performance by avoiding/reducing the \textit{turing tax}
    \item As a result, systems need to be able to efficiently use many different accelerators.
\end{itemize}

\begin{tcbraster}[raster columns=2,raster equal height]
    \begin{definitionbox}{GPU}
        \textit{Graphics Processing Unit}, designed for highly parallel operations on data (operating the same instructions across many threads in many warps).
    \end{definitionbox}
    \begin{definitionbox}{TPU}
        \textit{Tensor Processing Unit}, developed by Google for low precision arithmetic on tensors (matrices are 2D tensors)
    \end{definitionbox}
    \begin{definitionbox}{ASIC}
        \textit{Application-specific Integrated Circuit}. An IC designed to compute a specific application and hence with virtually no associated \textit{turing tax} overhead.
    \end{definitionbox}
    \begin{definitionbox}{Near Memory Computing}
        Accelerators built into/physically adjacent to main memory to avoid the bandwidth limitations of CPU memory access over a memory bus.
    \end{definitionbox}
\end{tcbraster}
\begin{definitionbox}{Field Programmable Gate Array (FPGA)}
    An array of programmable blocks that can be configured to a specific design (described by a developer using a \textit{hardware description language}) to perform a specific algorithm.
\end{definitionbox}
\centerline{\textit{Data Model Heterogeneity is Increasing}}
Many new data models have been developed to support specific types of application.
\begin{itemize}
    \item Key value stores used to improve performance of distributed systems through caching.
    \item Graph based models for highly interconnected data (e.g social networks) that avoid the costs associated with joins on very large relations
    \item Document based databases for flexibility \& simplicity in storing data (e.g storing BSON objects in MongoDB to support simple webapps)
\end{itemize}

\begin{tcbraster}[raster columns=2,raster equal height]
    \begin{sidenotebox}{Redis}
        \href{https://redis.io/}{Redis} is a popular in-memory key value store, often used as a cache but also usable as a key-value database.
    \end{sidenotebox}
    \begin{sidenotebox}{Memcached}
        \href{https://github.com/memcached/memcached}{Memcached} is a distributed key-value store designed for caching. Usage is nicely explained in this \href{https://github.com/memcached/memcached/wiki/TutorialCachingStory}{funny story}.
    \end{sidenotebox}
\end{tcbraster}
\begin{sidenotebox}{RedisGraph}
    A graph based database \href{https://redis.com/modules/redis-graph/}{RedisGraph} which uses adjacency matrices \& smart linear algebra to achieve a self-proclaimed title of \textit{fastest graph database}.
\end{sidenotebox}
\centerline{\textit{Workload Heterogeneity is Increasing}}
Datasets are growing larger with more kinds of workload.
\begin{center}
    \begin{tabular}{c c c c c c}
        Analytics & Transaction Processing & Inference & Data Cleaning & Data Integration \\
    \end{tabular}
\end{center}
\begin{itemize}
    \item Data integration workloads are required for the large distributed data systems
    \item Data science related workloads needed at scale (cleaning, model training and inference)
\end{itemize}

\section{CodeGen}
A typical DBMS implementation converts queries to logical, then physical plans. The kernel then invokes operator implementations specified in a query's physical plan to process the query.
\begin{center}
    \includegraphics[width=\textwidth]{advanced_topics/images/naive_query_path.drawio.png}
\end{center}
There are several unavoidable costs/limitations to optimisation:
\begin{itemize}
    \item With volcano processing we must compose/stitch together operators at runtime, necessitating expensive virtual calls.
    \item Inter-Operator microarchitectural optimisations (e.g inlining volcano operators) are not possible as the kernel can only use operator implementations, not edit/optimise/restructure their code
\end{itemize}
Alternatively we could generate the code for operator implementations at query time, with all the information available at that time.
\begin{center}
    \includegraphics[width=\textwidth]{advanced_topics/images/query_compilation_path.drawio.png}
\end{center}
There are optimisations that require both data and hardware awareness, particularly relating to parallelism (needs to understand data dependencies as well as the parallelism supported by hardware).
\begin{center}
    \includegraphics[width=\textwidth]{advanced_topics/images/unified_algebra_path.drawio.png}
\end{center}

\begin{definitionbox}{voodoo}
    A \textit{Vector-Dataflow Language} used as a unified algebra for code generating DBMS
    \href{https://www.cs.albany.edu/~jhh/courses/readings/pirk.pvldb16.pdf}{original paper}
\end{definitionbox}

\subsection{Vector Operations}
\[\begin{bmatrix}
        x_1, & x_2, & \dots, & x_n \\
    \end{bmatrix} + \begin{bmatrix}
        y_1, & y_2, & \dots, & y_n \\
    \end{bmatrix} = \begin{bmatrix}
        x_1 + y_1, & x_2 + y_2, & \dots, & x_n + y_n \\
    \end{bmatrix}\]
A single instruction/\textit{operation} (e.g $+$) operating on multiple data.
\begin{itemize}
    \item As each operation is independent, each can be performed in parallel.
    \item A very large vector can be partitioned and processed on multiple threads.
    \item To take advantage of this parallelism in a single thread, we can use vector extensions.
    \item Vector extensions include wider registers, and special instructions for operating on \textit{lanes} of a vector register in parallel.
\end{itemize}
\noindent For example element-wise sum over two tables (e.g previously joined in the plan).
\begin{minted}{SQL}
CREATE TABLE numbers (x BIGINT, y BIGINT); /* ... */; SELECT (x + y) as z FROM numbers;
\end{minted}
\begin{center}
    \includegraphics[width=\textwidth]{advanced_topics/images/vector_arithmetic.drawio.png}
\end{center}
Naively we could generate some \textit{scalar} code to perform the operation.
\begin{minted}{cpp}
template<size_t n>
void element_sum_scalar(int64_t x[n], int64_t y[n], int64_t z[n]) {
    for (auto i = 0; i < n; i++) z[i] = x[i] + y[i];
}
// Note: with optimisation on clang & gcc will automatically vectorize this
\end{minted}
We could use multithreading.
\begin{minted}{cpp}
template <size_t n>
void element_sum_threads(int64_t x[n], int64_t y[n], int64_t z[n]) {
    //number of concurrent threads supported
    const auto no_threads = std::thread::hardware_concurrency();
    
    // round-up integer division to get elements computed per thread
    const auto n_per_thread = ((n - 1) / no_threads) + 1;
    
    std::vector<std::thread> threads;
    
    for (auto index = 0; index < n; index += n_per_thread) {
        threads.emplace_back([&, index] {
        for (auto s = index; s < std::min(index + n_per_thread, n); s++)
            z[s] = x[s] + y[s];
        });
    }

    for (auto &t : threads) t.join();
}
\end{minted}
\noindent
Or we can use a vector extension we know is available on the hardware the system is running on, such as AVX-512 used here.
\begin{minted}{cpp}
#include <immintrin.h> // intel intrinsics used to ensure we use 512 bit vector instructions
#include <type_traits> // using enable_if as this code only works for n that are multiples of 8

template<size_t n>
typename std::enable_if<n % 8 == 0, void>::type 
element_sum_vec(int64_t x[n], int64_t y[n], int64_t z[n]) { 
    for (auto i = 0; i < n; i+=8) {
        __m512i xs = _mm512_loadu_si512(&x[i]);
        __m512i ys = _mm512_loadu_si512(&y[i]);
        __m512i zs = _mm512_add_epi64(xs, ys);
        _mm512_storeu_si512(&z[i], zs);
    }
}
\end{minted}
Given some call to \mintinline{cpp}{element_sum_vec<2048>(x, y, z)} we can compile:
\begin{minted}{bash}
g++ -O3 -mavx512f vectorisation.cpp  # Compiled with mavx512f to let GCC use AVX-512 instructions
\end{minted}
And extract the loop doing the summation:
\begin{minted}{nasm}
# Arrays stack allocated
.element_sum_vec:
    xor     eax, eax
.Loop:
    vmovdqu64  zmm1, ZMMWORD PTR [rsp+rax]          # xs = x[i:i+8]
    vpaddq     zmm0, zmm1, ZMMWORD PTR [r13+0+rax]  # zs = xs + y[i:i+8]
    vmovdqu64  ZMMWORD PTR [rbx+rax], zmm0          # z[i:i+8] = zs

    add rax,    64   # i += 8    * sizeof(int64_t)
    cmp rax, 16384   # i != 2048 * sizeof(int64_t)
    jne .Loop
    ret
\end{minted}

\subsection{Data Flow}
\begin{center}
    \includegraphics[width=\textwidth]{advanced_topics/images/voodoo.drawio.png}
\end{center}
\textit{Voodoo} expresses plans as a data-flow graph containing vector operations (which it can parallelise with multithreading, SIMD or GPU).

\section{Adaptive Indexing}
\subsection{Cracking}
\begin{minted}{sql}
SELECT * FROM table WHERE x BETWEEN c1 AND c2; -- Given constants c1 and c2
\end{minted}
\begin{center}
    \begin{tabular}{l p{.8\textwidth}}
        \textbf{Scan}         & Linearly scan table, get entries.                                                                              \\
        \textbf{Sorted Index} & Build a sorted index, maintain the index under writes, inserts \& deletes. Use index to get range efficiently. \\
        \textbf{Cracking}     & Split/\textit{crack} the table into several unsorted ranges, with the ranges in sorted order.                  \\
    \end{tabular}
\end{center}
\begin{center}
    \includegraphics[width=\textwidth]{advanced_topics/images/cracked_table.drawio.png}
\end{center}
Cracking has been shown to significantly improve performance, as examined in the paper \href{https://stratos.seas.harvard.edu/files/IKM_CIDR07.pdf}{\textbf{Database Cracking}} (benchmarking cracking in monetDB).
\begin{center}
    \includegraphics[width=.9\textwidth]{advanced_topics/images/cracking_perf.png}
\end{center}

\begin{sidenotebox}{"Cracking stuff Gromit!"}
    \begin{itemize}
        \item \href{https://homepages.cwi.nl/~mk/onderwijs/adt2009/lectures/lecture5.pdf}{These slides} cover the cracking implementation in monetDB.
        \item \href{https://stratos.seas.harvard.edu/sites/scholar.harvard.edu/files/StochasticDatabaseCrackingPVLDB12.pdf}{Stochastic Database Cracking: Towards Robust Adaptive
                  Indexing in Main-Memory Column-Stores}
    \end{itemize}
\end{sidenotebox}

\subsection{Hoare Partitioning}
\begin{center}
    \includegraphics[width=\textwidth]{advanced_topics/images/hoare_partitioning.drawio.png}
\end{center}
The partitioning algorithm used in quicksort
\begin{itemize}
    \item $O(n)$ time complexity
    \item Does not require extra memory / partitions in-place.
\end{itemize}

\begin{minted}{cpp}
// partition part of a vector in range [start-inc, end-exc) and return the pivot index
template <typename T>
size_t partition(std::vector<T> &sort_vec, size_t start, size_t end) {
  // get pivot
  T pivot = sort_vec[start];
  size_t count = 0;

  // determine where to partition / where to place pivot value
  for (size_t i = start + 1; i < end; i++) {
    if (sort_vec[i] <= pivot)
      count++;
  }
  
  // swap pivot into place, will partition around pivot
  size_t pivotIndex = start + count;
  std::swap(sort_vec[pivotIndex], sort_vec[start]);
  
  // start pointers i & j at ends of range
  size_t i = start, j = end - 1;
  
  // advance pointers, swap and partition
  while (i < pivotIndex && j > pivotIndex) {
    while (sort_vec[i] <= pivot) i++;
    while (sort_vec[j] > pivot) j--;

    if (i < pivotIndex && j >= pivotIndex) {
      std::swap(sort_vec[i], sort_vec[j]);
      i++;
      j--;
    }
  }

  return pivotIndex;
}
\end{minted}

Consider the following section from the algorithm.
\begin{minted}{cpp}
while(sort_vec[i] <= pivot) i++;
\end{minted}
A \textit{hot loop} containing a conditional with low selectivity (on random data).
\begin{itemize}
    \item we can employ an out-of-place algorithm that allows us to remove this \textit{control hazard}
\end{itemize}

\subsection{Predication}
\begin{sidenotebox}{Predication}
    The idea behind predication is to avoid \textit{control hazards}.
    Some architectures support this directly with predicated instructions
    \begin{minipage}[t]{.3\textwidth}
        \begin{minted}{cpp}
if (a > b) a += b;
        \end{minted}
    \end{minipage} \hfill \begin{minipage}[t]{.2\textwidth}
        \begin{minted}{asm}
    cmp r0, r1
    addlt r0, r0, r1


    
        \end{minted}
    \end{minipage} \hfill\begin{minipage}[t]{.2\textwidth}
        \begin{minted}{asm}
    cmp r0, r1
    ble dont_add 
    add r0, r0, r1
dont_add:
    @ ...
        \end{minted}
    \end{minipage}
\end{sidenotebox}
We can start with a basic out-of-place partition.
\begin{minted}{cpp}
template<std::copy_constructible T>
size_t partition(const std::vector<T>& input_vec, std::vector<T>& output_vec, size_t start, size_t end)
{
    const T& pivot = input_vec[(start + end) / 2];  
    size_t left_index = start;
    size_t right_index = end - 1;

    for (auto i = start; i < end; i++) {
        if (input_vec[i] < pivot) {
            output_vec[left_index] = input_vec[i];
            left_index++;
        } else {
            output_vec[right_index] = input_vec[i];
            right_index--;
        }
    }

    return right_index;
}
\end{minted}
Here the \mintinline{cpp}{if (input_vec[i] < pivot)} condition has low selectivity, and is part of the hot loop.
\\
\\ We can predicate this by always writing the \mintinline{cpp}{input_vec[i]}, and incrementing the pivot indexes based on the condition.
\begin{minted}{cpp}
template<std::copy_constructible T>
size_t partition(const std::vector<T>& input_vec, std::vector<T>& output_vec, size_t start, size_t end)
{
    const T& pivot = input_vec[(start + end) / 2];  
    size_t left_index = start;
    size_t right_index = end - 1;

    for (auto i = start; i < end; i++) {
        output_vec[left_index] = input_vec[i];
        output_vec[right_index] = input_vec[i];

        // increment using boolean, if not incremented, value is overwritten on the next iteration 
        // of the loop 
        left_index += input_vec[i] < pivot;
        right_index -= input_vec[i] >= pivot;
    }

    return right_index;
}
\end{minted}

\subsection{Predicated Cracking}



\section{Stream Processing}
\section{Composable Data Processing}

