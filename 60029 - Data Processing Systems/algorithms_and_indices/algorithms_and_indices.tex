\chapter{Algorithms and Indices}
\begin{sidenotebox}{Play \& Contribute!}
    C++ implementations, tests and benchmarks for this section are included in the associated code directory for this chapter.
\end{sidenotebox}

\section{Sorting Algorithms (unassessed)}

\subsection{Quicksort}
\begin{center}
    \includegraphics[width=.9\textwidth]{algorithms_and_indices/images/quicksort.drawio.png}
\end{center}
\inputminted{cpp}{algorithms_and_indices/code/sort_comparison/sorts/quicksort.h}
\begin{center}
    \begin{tabular}{c | c}
        \textbf{Average Complexity} & \textbf{Worst-Case Complexity} \\
        $O(n \log n)$               & $O(n^2)$                       \\
    \end{tabular}
\end{center}
Selecting a balanced pivot (ideally the median) is important to avoid worst-case complexity (where all others are larger or smaller than the pivot). Sampling multiple possible pivots negates this, as do other hybrid sorts.

\begin{tabbox}{prosbox}
    \textbf{Quick} & On average typically faster than merge or heapsort. \\
    \textbf{In-Place} & Sort can be performed entirely in place (no extra memory required, good temporal locality). \\
    \textbf{Parallel} & Is trivial to parallelise. \\
\end{tabbox}
\begin{tabbox}{consbox}
    \textbf{Worst-Case} & Avoiding $O(N^2)$ \\
    \textbf{Blind} & Does not take advantage of partially sorted data (in fact this can lead to worst-case depending on pivot selection). \\
\end{tabbox}

\subsection{Merge Sort}
\begin{center}
    \includegraphics[width=\textwidth]{algorithms_and_indices/images/mergesort.drawio.pdf}
\end{center}
\inputminted{cpp}{algorithms_and_indices/code/sort_comparison/sorts/mergesort.h}
\begin{center}
    \begin{tabular}{c | c}
        \textbf{Average Complexity} & \textbf{Worst-Case Complexity} \\
        $O(n \log n)$               & $O(n \log n)$                  \\
    \end{tabular}
\end{center}
\begin{tabbox}{prosbox}
    \textbf{Worst-Case} & Same time complexity as best-case. \\
    \textbf{Parallel} & Is trivial to parallelise. \\
\end{tabbox}
\begin{tabbox}{consbox}
    \textbf{Out-of-Place} & Extra memory required for merges / cannot be done entirely in place \& worse locality. \\
    \textbf{Blind} & Does not take advantage of partially sorted data. \\
\end{tabbox}
\begin{sidenotebox}{Bring order to the galaxy!}
    The provided simplistic mergesort allocates many vectors for out-of-place merging. Try and improve it!
    \begin{itemize}
        \item Reuse one scratch vector for out-of-place merging
        \item Don't copy the single element \textit{splits} at the merge sort's base case.
        \item Predicate the merge process - see \autoref{section:predication}.
    \end{itemize}
\end{sidenotebox}

\subsection{Heapsort}
\begin{center}
    \includegraphics[width=.85\textwidth]{algorithms_and_indices/images/heapsort.drawio.png}
\end{center}
\inputminted{cpp}{algorithms_and_indices/code/sort_comparison/sorts/heapsort.h}
\begin{center}
    \begin{tabular}{c | c}
        \textbf{Average Complexity} & \textbf{Worst-Case Complexity} \\
        $O(n \log n)$               & $O(n \log n)$                  \\
    \end{tabular}
\end{center}
\begin{tabbox}{prosbox}
    \textbf{Worst-Case} & Same time complexity as best-case. \\
    \textbf{Top-N} & If we only want to $n$ largest items, heapsort can extract this without sorting the entire array. \\
    \textbf{In-Place} & Sort can be performed entirely in place (no extra memory required, good temporal locality). \\
\end{tabbox}
\begin{tabbox}{consbox}
    \textbf{Parallelism} & Difficult to parallelize. \\
    \textbf{Blind} & Does not take advantage of partially sorted data. \\
\end{tabbox}
\begin{sidenotebox}{Bring order to the galaxy!}
    There are parallelised heapsort implementations, such as \href{Dualheap Sort}{https://arxiv.org/ftp/arxiv/papers/0706/0706.2893.pdf}.
\end{sidenotebox}

\subsection{Radix Sort}
\begin{center}
    \includegraphics[width=\textwidth]{algorithms_and_indices/images/radixsort.drawio.png}
\end{center}
A non-comparative sorting algorithm. Rather than comparing elements to determine an order, continually split buckets by some place.
\begin{center}
    \begin{tabular}{c}
        \textbf{Worst-Case Complexity}            \\
        $O(\text{key width} \times \text{radix})$ \\
    \end{tabular}
\end{center}
\begin{tabbox}{prosbox}
    \textbf{Parallelism} & Trivially parallel. \\
    \textbf{In-Place} & In-place variants of the sort exist. \\
\end{tabbox}
\begin{tabbox}{consbox}
    \textbf{Inefficient on Small Data} & Often paired with another sort (e.g insertion sort) used when buckets are small. \\
    \textbf{Lexicographical Only} & Data must have a lexicographical order (e.g integers) \\
\end{tabbox}

\subsection{Hybrid Sorts}
Sorts can be combined to avoid worst-case weaknesses. A typical pattern is to pair quicksort with another sort switch to to avoid quicksort's poor worst-case complexity.
\begin{center}
    \begin{tabular}{l | p{.7\textwidth}}
        \textbf{Pattern Defeating Quicksort} & A hybrid sort using quicksort's fast average complexity, and heapsort's good worst-case complexity. \\
        \textbf{Multi-Pivot Quicksort}       & For improved cache performance (a dual-pivot quicksort was used in Java 7).                         \\
        \textbf{Quick-Radix Sort}            & Partitions based on bit (depth of recursion) of the key.                                            \\
        \textbf{Timsort}                     & A combination of merge sort and insertion sort, originally developed for python.                    \\
    \end{tabular}
\end{center}

\section{Joins}
\begin{definitionbox}{Database Normalisation}
    Structuring a relational database according to progressively stricter \textit{normal forms} to remove redundant data, and improve integrity (prevent potential logical inconsistency under insertion/update/deletion).
    \\ \href{https://en.wikipedia.org/wiki/Database_normalization}{See wikipedia}.
\end{definitionbox}

\subsection{Join Types}
Normalised databases naturally require joins to re-compose data.

\begin{definitionbox}{Join}
    A join is a cross product with selection using data from both relations ($\sigma_{p(R_A.x, R_B.y)} (R_A \times R_B)$).
\end{definitionbox}

\subsubsection{Inner Joins}
\begin{tcbraster}[raster columns=2, raster equal height]
    \begin{definitionbox}{Inner Join}
        A join only returning rows from both tables which satisfy a predicate/condition.
        \begin{center}
            \begin{tikzpicture}[fill=gray]
                % left hand
                \scope
                \clip (1,0) circle (1);
                \fill (0,0) circle (1);
                \endscope
                % right hand
                \scope
                \clip (0,0) circle (1);
                \fill (1,0) circle (1);
                \endscope
                % outline
                \draw (0,0) circle (1) (0,1)  node [text=black,above] {$R_1$}
                (1,0) circle (1) (1,1)  node [text=black,above] {$R_2$};
            \end{tikzpicture}
        \end{center}
    \end{definitionbox}
    \begin{definitionbox}{Natural Join}
        Joining two tables with an implicit join clause (join on equality on a column present in both tables
        \[R_1 \bowtie R_2\]
        \begin{minted}{sql}
FROM R1 NATURAL JOIN R2
FROM R1 JOIN R2 USING(id)
        \end{minted}
    \end{definitionbox}
\end{tcbraster}
\begin{tcbraster}[raster columns=2, raster equal height]
    \begin{definitionbox}{Theta Join}
        Joining two tables based on a condition/predicate $\theta$.
        \[R_1 \overset{\theta}{\bowtie} R_2\]
        \begin{minted}{sql}
FROM R1, R2 WHERE theta(R1, R2)
FROM R1 JOIN R2 ON theta(R1, R2)
        \end{minted}
    \end{definitionbox}
    \begin{definitionbox}{Equi Join}
        A \textbf{theta join} with a single equivalence condition. A \textbf{natural join} is an implicit \textbf{equi join}.
        \[R_1 \bowtie_{R_1.x = R_2.x}\]
        \begin{minted}{sql}
FROM R1, R2 WHERE R1.x = R2.x
        \end{minted}
    \end{definitionbox}
\end{tcbraster}
\begin{tcbraster}[raster columns=2, raster equal height]
    \begin{definitionbox}{Cross Join}
        Just cartesian product with no selection.
        \[R_1 \times R_2\]
        \begin{minted}{sql}
    FROM R1, R2
    FROM R1 CROSS JOIN R2
        \end{minted}
    \end{definitionbox}
    \begin{definitionbox}{Anti Join}
        A \textbf{theta join} using an inequality predicate
        \[R_1 \bowtie_{R_1.x <> R_2.x} R_2\]
        \begin{minted}{sql}
FROM R1 JOIN R2 ON R1.x <> R2.x
        \end{minted}
    \end{definitionbox}
\end{tcbraster}

\subsubsection{Outer Joins}
\begin{tcbraster}[raster columns=2, raster equal height]
    \begin{definitionbox}{Left Join}
        \[R_1 \overset{L}{\bowtie} R_2\]
        Returns all rows of $R_1$ even if no rows in $R_2$ match (in which case columns are \mintinline{sql}{NULL}).

        \begin{center}
            \begin{tikzpicture}[fill=gray]
                % left hand
                \scope
                \fill (0,0) circle (1);
                \endscope
                % right hand
                \scope
                \clip (0,0) circle (1);
                \fill (1,0) circle (1);
                \endscope
                % outline
                \draw (0,0) circle (1) (0,1)  node [text=black,above] {$R_1$}
                (1,0) circle (1) (1,1)  node [text=black,above] {$R_2$};
            \end{tikzpicture}
        \end{center}
        \begin{minted}{sql}
FROM R1 LEFT JOIN R2 ON ... 
            \end{minted}
    \end{definitionbox}
    \begin{definitionbox}{Right Join}
        \[R_1 \overset{R}{\bowtie} R_2\]
        Returns all rows of $R_2$ even if no rows in $R_1$ match (in which case columns are \mintinline{sql}{NULL}).
        \begin{center}
            \begin{tikzpicture}[fill=gray]
                % left hand
                \scope
                \clip (1,0) circle (1);
                \fill (0,0) circle (1);
                \endscope
                % right hand
                \scope
                \fill (1,0) circle (1);
                \endscope
                % outline
                \draw (0,0) circle (1) (0,1)  node [text=black,above] {$R_1$}
                (1,0) circle (1) (1,1)  node [text=black,above] {$R_2$};
            \end{tikzpicture}
        \end{center}
        \begin{minted}{sql}
FROM R1 RIGHT JOIN R2 ON ... 
            \end{minted}
    \end{definitionbox}
\end{tcbraster}
\begin{definitionbox}{Full Outer Join}
    \[R_1 \overset{O}{\bowtie} R_2 \equiv R_1 \overset{L}{\bowtie} R_2 \cup R_1 \overset{R}{\bowtie} R_2\]
    Returns all rows from all tables matching, with rows from either $R_1$ or $R_2$ that do not have match associated with \mintinline{sql}{NULL} columns from the other table.
    \begin{center}
        \begin{tikzpicture}[fill=gray]
            % left hand
            \scope
            \fill (0,0) circle (1);
            \endscope
            % right hand
            \scope
            \fill (1,0) circle (1);
            \endscope
            % outline
            \draw (0,0) circle (1) (0,1)  node [text=black,above] {$R_1$}
            (1,0) circle (1) (1,1)  node [text=black,above] {$R_2$};
        \end{tikzpicture}
    \end{center}
    \begin{minted}{sql}
FROM R1 FULL OUTER JOIN R2 ON ... 
FROM R1 FULL JOIN R2 ON ... 
FROM (SELECT * FROM R1 LEFT JOIN R2 ON ... UNION SELECT * FROM R1 RIGHT JOIN R2 ON ...)
    \end{minted}
\end{definitionbox}


\begin{examplebox}{Which imposter?}
    Which of the following are joins?
    \begin{multicols}{2}
        \begin{enumerate}
            \item {
                  \begin{minted}{sql}
SELECT R.r, S.s
FROM R, S
WHERE R.id = S.id;
\end{minted}
                  }
            \item {
                  \begin{minted}{sql}
SELECT R.r, S.s
FROM R, S
WHERE R.r = R.id
    \end{minted}
                  }
            \item {
                  \begin{minted}{sql}
SELECT R.r
FROM R, S
WHERE R.id = S.id;
        \end{minted}
                  }
            \item {
                  \begin{minted}{sql}
SELECT R.r
FROM R, S
WHERE R.r = "some string";
            \end{minted}
                  }
        \end{enumerate}
    \end{multicols}
    \tcblower
    \begin{enumerate}
        \item \textbf{Join} (Selects on both $R$ and $S$)
        \item \textbf{Not a Join} (Only selects on $R$)
        \item \textbf{Join} (The $\sigma$ selection is on $R$ and $S$, so a join even if only $R$ is projected)
        \item \textbf{Not a Join} (Only selects on $R$)
    \end{enumerate}
\end{examplebox}

\subsection{Join Implementations}
\subsection{Nested Loop Join}
We can implement a basic join naively using nested loops.
\inputminted[firstline=6, lastline=19]{cpp}{algorithms_and_indices/code/join_comparison/joins/nested_loop.h}
\[\text{Time Complexity} = \begin{cases}
        \cfrac{\Theta (|left| \times |right|) }{2} & \text{ If elements unique} \\
        \Theta (|left| \times |right|)             & otherwise                  \\
    \end{cases}\]
\begin{tabbox}{prosbox}
    \textbf{Simple} & Easy to reason about (memory accesses \& complexity) \\
    \textbf{Trivially Parallel} & Loop iterations are not dependent, so can be parallelised. \\
    \textbf{Sequential I/O} & Access is done in the order of the tables storage (sequential access better for both memory \& disk) \\
\end{tabbox}
\begin{tabbox}{consbox}
    \textbf{Performance} & Linear time complexity. \\
\end{tabbox}

\subsection{Sort Merge Join}
If we assume both tables are sorted, and values (being joined on) are unique.
\begin{itemize}
    \item Two cursors (one per table)
    \item Advance cursors in order, if the value on the left exceeds the right there can be no joins for the left row (and vice versa).
\end{itemize}
\inputminted[firstline=10, lastline=44]{cpp}{algorithms_and_indices/code/join_comparison/joins/unique_sort_merge.h}
\begin{sidenotebox}{Non-Unique}
    A sort-merge that does not require uniqueness is included in this chapter's code directory.
\end{sidenotebox}

\[\begin{split}
        \text{Time Complexity} &= \Theta (sort(left)) + \Theta (sort(right)) + \Theta (merge) \\
        & = \Theta(|left| \times \log |left| + |right| \times \log |right| + |left| + |right|)
    \end{split}\]
\begin{tabbox}{prosbox}
    \textbf{Sequential I/O} & In the merge phase \\
    \textbf{Inequality} & Works for joins using $<$ and $>$ instead of just \textit{equi-joins}. \\
\end{tabbox}
\begin{tabbox}{consbox}
    \textbf{Tricky to Parallelize} & Sorts can be somewhat parallelised, but merge is sequential. \\
\end{tabbox}

\subsection{Hash Join}
For \textit{equi joins} we can insert one table into a hash table, then iterate over the second (assumed constant time lookup in hashtable).
\begin{itemize}
    \item Below we have used the standard template library's \mintinline{cpp}{unordered_map}
    \item We assume each value in a table is unique, otherwise a \mintinline{cpp}{unordered_multimap} is required.
\end{itemize}
\inputminted[firstline=9, lastline=29]{cpp}{algorithms_and_indices/code/join_comparison/joins/unique_hash.h}
\[\begin{split}
        \Theta (|build| + |probe|) & \text{best case} \\
        O(|build| \times |probe|) & \text{worst case} \\
    \end{split}\]
\begin{itemize}
    \item The probing phase can be easily parallelised (hashtable is unchanged), however the build side is tricky to paralleliuse efficiently.
\end{itemize}
\begin{tabbox}{prosbox}
    \textbf{Time Complexity} & (Assuming the lookup is constant time). \\
    \textbf{Hashing} & Need to avoid collisions, keep time calculating hash low, and be applicable to many data types. \\
\end{tabbox}
\begin{tabbox}[.7\textwidth]{consbox}
    \textbf{Space Complexity} & Requires building a hashtable structure (assumning the table was not stored as this already). Best when one relation is much smaller than the other (use smallest). \\
    \textbf{Expensive Hashing} & Some good hashing algorithms are expensive (potentially as many cycles as multiple data accesses). \\
\end{tabbox}

\begin{sidenotebox}{Bucket Based Hashmaps}
    Many hashmaps are implemented as a table of buckets (linked lists of conflicting values).
    \begin{itemize}
        \item Called bucket-chaining/open addressing
        \item Poor lookup performance.
        \item Good insert performance (can prepend to bucket linked list on conflict).
    \end{itemize}
\end{sidenotebox}


\section{Hash Tables}
\subsection{Hashing}
A hash function is used to lookup keys in a hashmap.
\begin{minted}{cpp}
template<typename K> using Hasher = size_t (const K&);
\end{minted}
\begin{center}
    \begin{tabular}{l l p{.6\textwidth}}
        Requirement & \textbf{Pure}                 & no state/call with same value $\to$ same hash                 \\
        Requirement & \textbf{Known Co-Domain}      & Known range of values (co-domain also known as image/range).  \\
        Nicety      & \textbf{Contiguous Co-Domain} & No gaps in range of output means few gaps holes in the table. \\
        Nicety      & \textbf{Uniform}              & All hash values in the range are equally likely.              \\
    \end{tabular}
\end{center}

\begin{sidenotebox}{Typical Hashers}
    \begin{center}
        \begin{tabular}{l p{.8\textwidth}}
            \textbf{MD5}             & Encodes any length string as a 128-bit hash.                                                                                                                                                                                                                                                                                \\
            \textbf{Modulo-Division} & Very simple and fast.                                                                                                                                                                                                                                                                                                       \\
            \textbf{MurmurHash}      & A fast, non-cryptographic hash (\href{https://github.com/aappleby/smhasher}{on github}).                                                                                                                                                                                                                                    \\
            \textbf{CRC32}           & Cyclic Redundancy Check (common, non-cryptographic) and with hardware support on some systems (also see usage of \mintinline{asm}{PCLMULQDQ} on intel for acceleration \href{https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/fast-crc-computation-generic-polynomials-pclmulqdq-paper.pdf}{here})
        \end{tabular}
    \end{center}
\end{sidenotebox}

\begin{examplebox}{Hash it out}
    Write a basic Modulo-Division hash using the interface above provided. Take the modulus as a template parameter.
    \tcblower
    \begin{minted}{cpp}
template <size_t MODULUS> size_t modulusHash(const int& data) {
  return static_cast<size_t>(data) % MODULUS;
}
    \end{minted}
\end{examplebox}

Hashes can collide, and hence we need a way to resolve this.

\subsection{Bucket Hashmap (Separate Chaining)}
Collisions are resolved using linked-list buckets.
\begin{center}
    \includegraphics[width=.9\textwidth]{algorithms_and_indices/images/bucket_hashmap.drawio.png}
\end{center}

\begin{tabbox}{prosbox}
    \textbf{Simple} & Easy to implement (no extra logic for collisions required, can just insert/delete from linked list buckets). \\
\end{tabbox}
\begin{tabbox}[.7\textwidth]{consbox}
    \textbf{Locality} & Linked list nodes are usually allocated separately, resulting in a random access pattern on larger buckets. \\
\end{tabbox}

\inputminted[firstline=21, lastline=135]{cpp}{algorithms_and_indices/code/hashtable_comparison/hashtables/bucket.h}

\begin{sidenotebox}{Other Structures}
    \begin{center}
        \begin{tabular}{l p{.7\textwidth}}
            \textbf{Balanced Tree}                                                                         & If keys are ordered, then a self-balancing tree can provide $\log (n) $ lookup, potentially better then a bucket based hashmap under many collisions (bucket based hashmap can deteriorate into iterating over large lists). \\
            \textbf{\href{https://en.wikipedia.org/wiki/Dynamic_perfect_hashing}{Dynamic Perfect Hashing}} & Each bucket is a second hashtable. Requires a perfect hash over $k^2$ slots (where the hashtables have $k$ slots).                                                                                                           \\
        \end{tabular}
    \end{center}
\end{sidenotebox}

\subsection{Probing Hashmap (Open Addressing)}
Collisions are resolved by retrying on a new slot (determined by a probe).
\begin{center}
    \includegraphics[width=\textwidth]{algorithms_and_indices/images/hash_table.drawio.png}
\end{center}
We can define an interface for a probe using a concept.
\begin{minted}{cpp}
template<typename State>
struct ProbeResult {
    size_t hash;
    State state;
};

template<typename K, typename Probe>
concept IsProbe = requires {
    { 
        Probe::initial(std::declval<const K&>()) 
    } noexcept -> std::same_as<ProbeResult<typename Probe::State>>;
    { 
        Probe::collision(std::declval<const ProbeResult<typename Probe::State>&>()) 
    } noexcept -> std::same_as<ProbeResult<typename Probe::State>>;
};
\end{minted}
We can then implement different probes, ideally with the following qualities:
\begin{center}
    \begin{tabular}{l p{.8\textwidth}}
        High Locality & When detecting a conflict, the real key is close/same page/line.                                                                                           \\
        Not too High! & Very high locality will result in parts of the hash table being saturated, and long probe chains.                                                          \\
        No Holes      & We want to avoid leaving holes/wasted memory (may be used by hash function, but if the probing function never accesses, they are likely to never be used). \\
    \end{tabular}
\end{center}

\subsubsection{Linear Probing}
Add some \mintinline{cpp}{DISTANCE} to the probe position, wrap around at the end of the buffer (here we assume the hashmap structure modulates our output).
\begin{minted}{cpp}
template<typename K, Hasher<K> hasher, size_t DISTANCE>
struct Linear {
    struct State {}; // no state required for linear hashing

    static ProbeResult<State> initial(const K& key) noexcept { 
        return { .hash=hasher(key), .state={} }; 
    }

    static ProbeResult<State> collision(const ProbeResult<State>& prev) noexcept { 
        return { .hash=prev.hash+DISTANCE, .state={} }; 
    }
};
\end{minted}
\begin{tabbox}{prosbox}
    \textbf{Simple} & Easy to reason about memory access pattern. \\
    \textbf{Locality} & Can alter \mintinline{cpp}{DISTANCE} to place values as \textit{adjacently} as we need. \\
\end{tabbox}
\begin{tabbox}[.7\textwidth]{consbox}
    \textbf{Long Probe-Chains} & From too much locality on adversarial input data (can input data to the table to create worse case conflicts (and hence probe chain length) scenario) \\
\end{tabbox}

\subsubsection{Quadratic Probing}
\[P, P + 1^2, P + 2^2, P + 3^2, \dots, P + n^2, \dots\]
\begin{itemize}
    \item Wrap around end of table.
    \item Variants exist (still use power of $2$ but can include linear and constant term)
\end{itemize}
\begin{minted}{cpp}
template<typename K, Hasher<K> hasher>
struct Quadratic {
    struct State {
        size_t original_hash;
        size_t collisions = 0;
    };

    static ProbeResult<State> initial(const K& key) noexcept { 
        size_t hash = hasher(key);
        return {
            .hash=hash, 
            .state={ .original_hash=hash, .collisions=0 }
        };
    }
    static ProbeResult<State> collision(const ProbeResult<State>& prev) noexcept { 
        size_t collisions = prev.state.collisions + 1;
        return {
            .hash=prev.state.original_hash + collisions * collisions, 
            .state={
                .original_hash=prev.state.original_hash, 
                .collisions=collisions
            }
        };
    }
};
\end{minted}
\begin{tabbox}{prosbox}
    \textbf{Simple} & Easy to reason about memory access pattern. \\
    \textbf{Locality} & for first probes is good. \\
\end{tabbox}
\begin{tabbox}{consbox}
    \textbf{Conflicts} & Experiences conflicts in first probes where is it similar to linear. \\
\end{tabbox}

\subsubsection{Rehashing}
In order to distribute nodes uniformly, use a has function to hash a conflicting position to find the next one.
\begin{minted}{cpp}
template<typename K, Hasher<K> hasher, Hasher<size_t> reHasher>
struct ReHash {
    struct State {};
    
    static ProbeResult<State> initial(const K& key) noexcept { 
        return ProbeResult{ .hash=hasher(key), .state={} }; 
    }
    
    static ProbeResult<State> collision(const ProbeResult<State>& prev) noexcept { 
        return { .hash=reHasher(prev.hash), .state={} }; 
    }
};
\end{minted}
\begin{tabbox}{prosbox}
    \textbf{Simple} & To implement \\
    \textbf{Reuse} & Can potentially reuse the hashing function. \\
\end{tabbox}
\begin{tabbox}{consbox}
    \textbf{Locality} & is poor as probes distributed uniformly. \\
    \textbf{Conflict Probability} & is constant (every probe may conflict with another element). \\
\end{tabbox}

\subsubsection{Resizing}
For the example above we have considered fixed-size hashmaps.
\begin{itemize}
    \item Hashtables are typically overallocated by factor $2$ (twice as many slots as expected input tuples).
    \item Table can be resized once it is larger than some capacity (will change hash of values, so must effectively rebuild hashmap)
    \item When determining cost we amortise (spread cost of resize over all inserts \& (for this module) assume this cost is constant per insert).
\end{itemize}
For this reason, hash-joins (using hash tables) are best when one of the joined relations is much smaller than the other.

\subsubsection{Deleting with Markers}
\begin{center}
    \includegraphics[width=.8\textwidth]{algorithms_and_indices/images/mark_deletion.drawio.png}
\end{center}

\subsubsection{Probing Hashmap Implementation}
\inputminted[firstline=98, lastline=247]{cpp}{algorithms_and_indices/code/hashtable_comparison/hashtables/probing.h}

\subsection{Partitioning}
Sequential accesses are cheaper than random accesses, as they can access the same page in memory \& thus share the cost of the initially expensive cold access.
\[\begin{split}
        c &= \text{cost of page-in } \\
        \cfrac{n}{pagesize_{OS}} \times c  &= \text{cost of sequentially accessing }n \text{ elements}\\
        \cfrac{c}{pagesize_{OS}} &= \text{cost of one access} \\
    \end{split}\]
In order to reduce the cost of accessing some data we can:
\begin{itemize}
    \item Increase the page size (huge pages).
    \item Make the access pattern \textit{more} sequential.
\end{itemize}
The access pattern for hashtables is typically random (often intentional by hash function).
\begin{definitionbox}{Hashtable Thrashing}
    Thrashing due to random access pattern associated with hashtable access, on a large hashtable.
    \begin{itemize}
        \item e.g high page IO when hash-joining two large relations, as the hashtable is too large to fit in the buffer pool.
    \end{itemize}
\end{definitionbox}

\begin{center}
    \includegraphics[width=.9\textwidth]{algorithms_and_indices/images/partitioning.drawio.png}
\end{center}
\begin{itemize}
    \item As each partition can be joined independently, we can do one at a time, and hence each partition's hash-join's hashtable can fit within the buffer pool.
    \item Alternatively, we could join all in parallel.
\end{itemize}
The partitioning function can be as simple as $key \mod No. Partitions$.

\subsection{Indexing}
We can use a secondary store of redundant data to speed up queries.
\begin{itemize}
    \item Denormalised (redundant) data is controlled by the DBMS.
    \item Can be created or removed without affecting the system (other than performance \& storage space).
    \item Semantically invisible to the user (cannot change semantics of queries).
    \item Can be used to speed up data access of some queries (e.g avoiding having to build a hashtable in hash join as it is already available).
    \item Occupy potentially considerable space.
    \item Must be maintained under updates.
    \item Must be considered by query optimiser.
\end{itemize}
\begin{definitionbox}{Clustered/Primary Index}
    An index storing all tuples of a table.
    \begin{itemize}
        \item Only one per table
        \item Can use more space than the table being indexed
        \item No redundant data / no duplicates within the index (only one copy for each tuple is indexed) (no consistency issues)
    \end{itemize}
\end{definitionbox}
\begin{definitionbox}{Unclustered/Secondary Index}
    Used to store pointers to tuples of a table.
    \begin{itemize}
        \item No limit on number of indexes
        \item Does not replicate data (the tuples pointed to in the table), but may replicate pointers (multiple pointers in index to the same tuple in the table) (some consistency issues)
    \end{itemize}
\end{definitionbox}
\subsubsection{SQL Indexes}
ANSI SQL supports the creation \& destruction of indexes by the user.
\begin{minted}{sql}
CREATE INDEX index_name ON table_name (column_1, column2, ...);
DROP INDEX index_name;
\end{minted}
\begin{itemize}
    \item Unclear what type of index is created
    \item No control over parameters (e.g hash table size)
\end{itemize}
The standard has been extended by SQL implementations to allow for finer control.
\begin{sidenotebox}{The elephant in the room}
    Among other DBMS, Postgres supports many types of index (\href{https://www.postgresql.org/docs/current/indexes.html}{documentation here})
    \begin{minted}{psql}
/* By default CREATE INDEX uses a B-Tree */
CREATE INDEX name ON table USING HASH (column);

/* It is even possible to only index certain parts of a table using a WHERE clause */
CREATE INDEX access_log_client_ip_ix ON access_log (client_ip)
WHERE NOT (client_ip > inet '192.168.100.0' AND
           client_ip < inet '192.168.100.255');
    \end{minted}
\end{sidenotebox}

\subsection{Hash Indexes}
An index backed by a hash table.
\begin{center}
    \includegraphics[width=.8\textwidth]{algorithms_and_indices/images/hash_index_clustered.drawio.png}
\end{center}
\begin{center}
    \includegraphics[width=.8\textwidth]{algorithms_and_indices/images/hash_index_unclustered.drawio.png}
\end{center}
Persistent hash tables may grow very large (overallocate) and need to be rebuilt to grow (can cause unexpected spike when an insert causes a rebuild).
\\
\\ Aside from the normal pros/cons of hash tables in general:
\begin{tabbox}[.7\textwidth]{prosbox}
    \textbf{Hash-Joins \& Aggregation} & Perform well and remove build phase (provided they index on the columns joining). \\
    \textbf{Equality Selection} & {Can reduce number of candidate columns if not all columns are indexed
            \begin{minted}{sql}
SELECT * FROM table_name WHERE column1 = some_value;
    \end{minted}
        } \\
\end{tabbox}

\begin{tabbox}{consbox}
    \textbf{Limited Applicability} & Not useful for queries not using equality. \\
\end{tabbox}

\subsection{Bitmap Indexing}
\begin{definitionbox}{Bit Vector}
    A sequence of 1 bit boolean values indicating some condition holds for indexes of another sequence.
    \[BV_{==3}([1,2,5,6,3,2,3,4]) = [0,0,0,0,1,0,1,0] \]
    \begin{itemize}
        \item Memory is byte addressable, and registers typically word-size (usually 32/64 bits).
        \item Some useful instructions (and compiler intrinsics) can be used.
        \item Can use SIMD instructions to operate on sections of a bitvector in parallel without using multithreading.
    \end{itemize}
\end{definitionbox}

\begin{definitionbox}{Bitmap Index}
    \begin{center}
        \includegraphics[width=.8\textwidth]{algorithms_and_indices/images/bitmap_index.drawio.png}
    \end{center}
    A collection of bitvectors on a column (each for a distinct value in the column).
    \begin{itemize}
        \item Need one bitvector per distinct value in the column
        \item Bitvectors usually disjoint (column can only be one distinct value at one time)
    \end{itemize}
    \[size(rows, distinct\_values) = \cfrac{rows \times distinct\_values}{8} \ bytes\]
\end{definitionbox}
On some systems we can create an index of arbitrary predicates, and to scan multiple bitmaps (using boolean operators on them).
\\
\\ The CPU operates in word size chunks of the bitvector. Hence we can easily check if all bits in a word size chunk (e.g 32 bits) are zero. We only need to iterate through this chunk if the chunk is non-zero.
\begin{minted}{cpp}
#include <cstddef>
#include <cstdint>
#include <iostream>
#include <vector>

using namespace std;

// scans a vector of 32 bit ints:
// - indexes each integer from LSB(0) to MSB(31)
// - does not consider endian-ness
// 100... 100... <=> [1,1]
vector<size_t> scan_bitmap(const vector<uint32_t> &bitvector) {
  vector<size_t> positions;
  size_t index = 0;
  for (auto elem : bitvector) {
    for (size_t small_index = 0; elem; small_index++, elem >>= 1) {
      if (elem & 1) {
        positions.push_back(index + small_index);
      }
    }
    index += 32;
  }
  return positions;
}
\end{minted}

\begin{tabbox}{prosbox}
    \textbf{Bandwidth} & Can scan a column with reduce memory bandwidth (e.g integers $\to$ bitmap index is $32$ times less). \\
    \textbf{Flexibility} & Can often use arbitrary predicates (e.g $< x$) to either turn a filter into a bitmap scan, or reduce time to scan (if $x < y$ an index $< x$ and help with a $< y$ filter). \\
\end{tabbox}

\subsubsection{Binned Bitmaps}
When there is are a high number of distinct values, but we do not want many bitvectors, we can create several bitvectors covering ranges of values.
\begin{itemize}
    \item The bitvectors ranges need to cover the entire domain.
    \item Smaller range $\to$ more precise and more useful for queries concerning data in that range, at the cost of more space used (more bitvectors)
    \item Not all ranges need to be the ame size, we can use the distribution of values to determine the ranges of the bins.
\end{itemize}
The \textit{false-positive rate} given a filter for $z$, and a bin of range $(x,y)$ where $x < x < y$, what proportion of the $1$s in the bitvector are not for the value $z$.
\begin{tcbraster}[raster columns=2, raster equal height]
    \begin{definitionbox}{Equi-Width}
        \[width = \cfrac{max(column) - min(column)}{number\_bins}\]
        Split range into several equal size bins. Useful for uniformly distributed (or near) data.
    \end{definitionbox}
    \begin{definitionbox}{Height Binning}
        All bins should contain the same number of values.
        \begin{itemize}
            \item Construction difficult (usually sort, determine quatiles on a sample)
            \item False-positive rate is value independent
            \item As table changes, may need to re-bin.
        \end{itemize}
    \end{definitionbox}
\end{tcbraster}

\subsubsection{Run Length Encoding}
\begin{center}
    \includegraphics[width=.8\textwidth]{algorithms_and_indices/images/rle.drawio.png}
\end{center}

\subsection{B-Trees}
Trees are well suited to the requirements of a database:
\begin{itemize}
    \item Good complexity for equality lookups ($\log(n)$ tree traversal)
    \item Easy to update (hash-tables can require a resize and cause a load spike on insert)
\end{itemize}
Typical balanced tree data structures such as red/black trees, AVL trees are unsuited as they have low fan-out
(require a large number of traversals to node spread across many pages $\to$ many page faults occur to fetch only a
few nodes). Databases are I/O bound (here the I/O is page faults).

\begin{definitionbox}{B-Tree}
    \begin{center}
        \includegraphics[width=.6\textwidth]{algorithms_and_indices/images/b_tree_node.drawio.png}
    \end{center}
    A high fan-out, balanced tree.
    \begin{itemize}
        \item Each non-root node must contain at least $\left\lfloor \cfrac{n-1}{2} \right\rfloor$ key/value pairs.
        \item The tree is kept balanced (all leaves at same depth)
        \item Time complexity for search, insert and delete is $O(\log n)$
    \end{itemize}
\end{definitionbox}

\subsection{B+ Trees}
\unfinished

\subsubsection{Maintaining Balance}
When a node overflows (full but value needs to be inserted), choose a splitting element and split values one either side into new nodes.
\\ \unfinished

\subsection{Foreign Key Indices}
Most joins are using a foreign key relation.
\begin{itemize}
    \item Constraint implies the number of matching tuples is 1 (foreign key $\to$ unique primary key)
    \item A foreign key indices effectively cache/save a join.
\end{itemize}
\begin{tabbox}{prosbox}
    \textbf{Cheap} & Use a small amount of space, only need to add an extra pointer when updating, no extra optimisation effort needed (other than \textit{if an fk-index exists, use it}). \\
    \textbf{Used} & Foreign key constraints usually are only created where users intend to join. \\
\end{tabbox}
