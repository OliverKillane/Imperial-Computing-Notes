\chapter{Hypothesis Testing}

\begin{definitionbox}{Hypothesis Test}
	Given two samples, determine if the difference is significant enough to suggest the parameters are different.
	\begin{itemize}
		\bullpara{Null Hypothesis}{No statistical relation, there is no evidence for a claim. ($H_0$)}
		\bullpara{Alternative Hypothesis}{There is a statistical relation. ($H_1$)}
	\end{itemize}
	We can partition the parameter space $\Theta$ into two disjoint sets $\Theta_0$ and $\Theta_1$ for the null and alternative hypotheses, which can be expressed as:
	\[H_0 \ : \ \theta \in \Theta_0 \ \text{  and  } \ H_1 \ : \ \theta \in \Theta_1\]
	(We are testing if based on a given sample, based onm the estimated parameter, if it is plausible the sample distribution is from another distribution)
	\begin{itemize}
		\bullpara{Simple Hypothesis}{Test that $\theta = \theta_0$}
		\bullpara{Composite Hypothesis}{Test that $\theta > \theta_0$ or $\theta < \theta_0$}
	\end{itemize}
\end{definitionbox}
	Typically a test is of the form:
	\[H_0 \ : \ \theta = \theta_0 \ \text{  versus   } H_1 \ : \ \theta \neq \theta_0\]
	Some tests are one-sided, for example:
	\[H_0 \ : \ \theta > \theta_0 \ \text{  versus   } H_1 \ : \ \theta < \theta_0\]
	To test the validity of $H_0$:
	\begin{enumerate}
		\item Choose a \keyword{test statistic} $T(\underline{X})$ to use on the data.
		\item Find a distribution $P_T$ under $H_0$ from the \keyword{test statistic}.
		\item Determine the rejection region (the region in which a result would invalidate $H_0$).
		\item Calculate the observed \keyword{test statistics} $t(\underline{x})$.
		\item If $t(\underline{x})$ is in the rejection region, reject $H_0$ and accept $H_1$, else retain $H_0$.
    \end{enumerate}
	\centerimage{width=0.6\textwidth}{hypothesis_testing/images/rejection_regions.drawio.png}
	The \keyword{significance level/Type 1 Error Rate} $\alpha \in (0,1) $ of as hypothesis test determines the size of the rejection regions.
	\begin{itemize}
		\bullpara{$\alpha \to 0$}{Less and less likely to reject $H_0$, rejection region samller, confidence in our result is lower - easier test.}
		\bullpara{$\alpha \to 1$}{More and more likely to reject $H_0$, rejection region larger, confidence higher - stricter test.}
	\end{itemize}
	The \keyword{p-value} of a test is the significance level threshold between rejection/acceptance of $H_0$ for a given test.

\begin{definitionbox}{Test Errors}
	\begin{itemize}
		\bullpara{Type 1}{Reject $H_0$ when it is actually true. $\alpha = P(T \in R|H_0)$
			\\ (significance is the probability of incorrectly rejecting the null hypothesis)}
		\bullpara{Type 2}{Accepting $H_0$ when $H_1$ is true. $\beta = P(T \not\in R | H_1)$
			\\ Probability a test statistic is not in the rejecting region, when $H_1$ is true.}
    \end{itemize}
\end{definitionbox}
\begin{definitionbox}{Test Power}
	The probability of correctly rejecting the null hypothesis
	\[Power = 1 - \beta = 1 - P(T \not\in R|H_1) = P(T \in R|H_1)\]
	For a given significance level:
	\[\alpha = P(T \in R| H_0)\]
	A good \keyword{test statistic} $T$ and \keyword{rejection region} $R$ will have a high \keyword{power}, the highest \keyword{power} test under $H_1$ is called the \keyword{most powerful}.
\end{definitionbox}
Given a control group (placebo) and a test group (given some pharmaceutical), we can test the hypotheis that the drug has an effect on survival rates.
\[\begin{split}
        H_0: & \text{ The drug has no effect - survival rates are the same.} \\
        H_1: & \text{ The drug has an effect - survival rates are different.} \\
    \end{split}\]
\section{Testing For Population Mean}
Sample mean belongs to a normal distribution (\keyword{Central Limit Theorem}):
\[\overline{X} \thicksim N\left( \mu, \cfrac{\sigma^2}{n}\right)\]
We have our two hypotheses:
\[H_0 \ : \ \mu = \mu_0 \ \ \text{  versus  } \ \ H_1 \ : \ \mu \neq \mu_0\]
We can derive a new distribution in terms of the standard normal:
\[Z = \cfrac{\overline{X} - \mu_0}{\sfrac{\sigma}{\sqrt{n}}} \thicksim N(0,1)\]
Hence for significance $\alpha$ (or confidence interval $1 - \alpha$) we can get the rejection/acceptance regions.
\[\Phi(1 - \alpha) = threshold \ \ \text{   results in acceptance region: } [-threshold, threshold] \]
Hence we can calculate $z$ for a given sample, and then determine if it is in the region, if it is then accept $H_0$, else rejected $H_0$ and accept $H_1$.
\begin{examplebox}{Weight of Crisp Packets (Known Variance)}
	A crisp manufacturer sells packets listed as having weight $454g$. From a sample size of 50, we get the mean weight of a bag as $451.22g$.
	\\
	\\ Assume the variance of bag weights is $70$. Is the observed sample consistent with the claim made by the company at the 5\% significance.
	\[\begin{split}
			H_0 &: \ \mu = 454g \\
			H_1 &: \ \mu \neq 454g \\
		\end{split}\]
	We have the following information:
	\[\begin{matrix}
			\overline{x} = 451.22g & \sigma^2 = 70 & n = 50 & \alpha = 0.05 \\
		\end{matrix}\]
	Hence we can state the hypothesized distribution of the sample mean:
	\[\overline{X} \thicksim N\left(454g, \cfrac{70}{50}\right)\]
	We can get this in terms of the standard normal distribution:
	\[Z = \cfrac{\overline{X} - 454}{\sfrac{\sqrt{35}}{5}} \thicksim N(0,1)\]
	At the $5\%$ significance, we have $2.5\%$ are each tail. Hence we get our critical value as $z(critical) = 0.975$, where $1.96$.
	\\
	\\ Hence the rejection region is:
	\[\begin{split}
			\cfrac{\overline{X} - 454}{\sfrac{\sqrt{35}}{5}} & < -1.96 \\
			\cfrac{\overline{X} - 454}{\sfrac{\sqrt{35}}{5}} & > 1.96 \\
		\end{split}\]
	Hence in order to accept $H_0$, $\overline{X}$ must be in the interval:
	\[451.6809 < \overline{X} < 456.3191\]
	As $\overline{x} = 451.22$ it is in the rejection region, hence at the $95\%$ significance there is sufficient evidence to reject the company's claim.
\end{examplebox}
\begin{examplebox}{Weight of Crisp Packets (UnKnown Variance)}
	A Crisp manufacturer sells packets listed as having weight $454g$. From a sample size of 50, we get the mean weight of a bag as $451.22g$.
	\\
	\\ Assume the variance of bag weights is $70$. Is the observed sample consistent with the claim made by the company at the 5\% significance?
	\tcblower
    \[\begin{split}
			H_0 &: \ \mu = 454g \\
			H_1 &: \ \mu \neq 454g \\
		\end{split}\]
	We have the following information:
	\[\begin{matrix}
			\overline{x} = 451.22g & n = 50 & \alpha = 0.05 \\
		\end{matrix}\]
	We first calculate the bias corrected sample variance:
	\[\begin{split}
			s_{n-1} &= \sqrt{
				\cfrac{1}{n-1}\sum_{i=1}^n(x_i - \overline{x})^2
			} \\
			&= \sqrt{70.502} \ (\text{Need to calculate from each observation in the sample})
		\end{split}\]
	Hence we can now use the \keyword{student's t distribution} with degrees of freedom $n - 1 = 49$.
	\[\cfrac{\overline{x} - \mu_0}{\sfrac{s_{n-1}}{\sqrt{n}}} \thicksim t_{49}\]
	For $\alpha = 5\%$ we take the tails as $0.025$, so use $t_{49, \ 0.975} \approx 2.01$.
	Hence will reject the regions:
	\[\begin{split}
			\cfrac{\overline{X} - 454}{\sfrac{\sqrt{70.502}}{5\sqrt{2}}} & < -2.01 \\
			\cfrac{\overline{X} - 454}{\sfrac{\sqrt{70.502}}{5\sqrt{2}}} & > 2.01 \\
		\end{split}\]
	Hence to accept $H_0$, $\overline{X}$ must be:
	\[451.6123 < \overline{x} < 456.3868\]
	Hence at the $5\%$ significance there is sufficient evidence to reject $H_0$ and accept $H_1$.
\end{examplebox}
\begin{examplebox}{Optimising Code}
	The previous code had a mean run time of $6s$. Following an optimisation a sample of runs is taken, with sample of size $16$, mean $5.8s$ and bias corrected sample standard deviation of $1.2s$. Is the new code faster?
	\tcblower
    Our test is as follows:
	\[H_0 \ : \ \mu \geq 6s \ (\text{mean time is same or larger}) \ \ \text{  versus  } \ \ H_1 \ : \ \mu < 6s \ (\text{mean time is lower})\]
	We have the following information:
	\[\begin{matrix}
			\overline{x} = 5.8 & s_{n-1} = 1.2s & n = 16 \\
		\end{matrix}\]
	Hence we have the distribution:
	\[\cfrac{\overline{X} - \mu}{\sfrac{s_{n-1}}{\sqrt{n}}} \thicksim t_{15}\]
	Hence we can use the significance (one ended/top tail) of $5\%$ to find $t_{15, 0.95} \approx 1.75$.
	\\
	\\ Hence will reject the regions:
	\[\begin{split}
			\cfrac{\overline{X} - 6}{\sfrac{1.2}{4}} & < -1.75 \\
			\cfrac{\overline{X} - 6}{\sfrac{1.2}{4}} & > 1.75 \\
		\end{split}\]
	Hence to accept $H_0$, $\overline{X}$ must be:
	\[5.475 < \overline{X} < 6.525\]
	Hence as $\overline{x} = 5.8$ this is within the acceptable region, so at the $95\%$ significance we have insufficient evidence to reject $H_0$.
\end{examplebox}
\section{Samples from Two Populations}
When given two random samples:
\[\underline{X} = (X_1, \dots, X_n) \ \text{from }P_X \ \text{   and   } \ \ \underline{Y} = (Y_1, \dots, Y_n) \ \text{from } P_Y\]
We may want to determine the similarity of the distributions of $P_X$ and $P_Y$.
\\
\\ Typically this involves testing to see if the means of the populations are equal:
\[H_0 \ : \ \mu_X = \mu_Y \ \ \text{  versus  } \ \ H_1 \ : \ \mu_X \neq \mu_Y\]
\begin{definitionbox}{Paired Data}
	A special case when $\underline{X}$ and $\underline{Y}$ are pairs $(X_1, Y_1), \dots, (X_n, Y_n)$ (each $X_i$ and $Y_i$ are possibly dependent on each-other).
	\\
	\\ For example, where for a person $i$, $X_i$ is the heart rate before exercise, and $Y_i$ the rate afterwards.
	\\
	\\ We can consider a sample of the differences, if this has mean $0$:
	\[Z_i = X_i - Y_i \ \text{  testing  } H_0 \ : \ \mu_Z = 0 \ \ \text{  versus  } \ \ H_1 \ : \ \mu_Z \neq 0\]
\end{definitionbox}
\subsection{Known Variance, $X$ and $Y$ are Independent}
Given that:
\[\begin{matrix}
		\underline{X} = (X_1, \dots, X_{n_1}) & X_i \thicksim N(\mu_X, \sigma^2_X) & \overline{X} \thicksim N\left(\mu_X, \cfrac{\sigma^2_X}{n_1}\right) \\
		\underline{Y} = (Y_1, \dots, Y_{n_2}) & Y_i \thicksim N(\mu_Y, \sigma^2_Y) & \overline{Y} \thicksim N\left(\mu_Y, \cfrac{\sigma^2_Y}{n_2}\right) \\
	\end{matrix}\]
We can therefore get the distribution of the difference in sample means:
\[\overline{X} - \overline{Y} \thicksim N\left(\mu_X - \mu_Y, \cfrac{\sigma_X^2}{n_1} + \cfrac{\sigma_Y^2}{n_2}\right)\]
And hence:
\[\cfrac{(\overline{X} - \overline{Y}) - (\mu_x - \mu_Y)}{\sqrt{\cfrac{\sigma^2_X}{n_1} + \cfrac{\sigma^2_Y}{n_2}}} \thicksim N(0, 1)\]
As we assume for $H_0$ that $\mu_x = \mu_Y$ we have:
\[\cfrac{\overline{X} - \overline{Y}}{\sqrt{\cfrac{\sigma^2_X}{n_1} + \cfrac{\sigma^2_Y}{n_2}}} \thicksim N(0, 1)\]
So we can calculate the \keyword{test statistic}:
\[z = \cfrac{\overline{x} - \overline{y}}{\sqrt{\cfrac{\sigma^2_X}{n_1} + \cfrac{\sigma^2_Y}{n_2}}}\]
And use this to determine if $H_0$ is rejected.
\subsection{Unknown Variance, $X$ and $Y$ are Independent, Variances Equal}
\begin{definitionbox}{Bias-Corrected Pooled Sample Variance}
If the variance of two samples is the same, given:
\[\underline{X} = (X_1, \dots, X_{n_1}) \ \text{  and  } \ \underline{Y} = (Y_1, \dots, Y_{n_2})\]
We can get an unbiased estimator of the variance as:
\[S^2_{N_1 + n_2 - 2}\cfrac{(n_1 - 1)S^2_{n_1 - 1, \ X} + (n_2 - 1)S^2_{n_2 - 1, \ Y}}{(n_1 - 1) + (n_2 - 1)}\]
Which is equivalent to:
\[S^2_{n_1 + n_2 - 2} = \cfrac{\sum_{i=1}^{n_1}(X_i - \overline{X})^2 + \sum_{i=1}^{n_2}(Y_i - \overline{Y})^2}{n_1 + n_2 - 2}\]
\end{definitionbox}
If $\sigma^2_X$ and $\sigma^2_Y$ are unknown, but it is know that $\sigma^2 = \sigma^2_X = \sigma^2_Y$ we can use an estimator to get an estimate of the variance $\sigma^2$ using the samples from the two populations.
\[\cfrac{(\overline{X} - \overline{Y}) - (\mu_x - \mu_Y)}{\sigma \sqrt{\sfrac{1}{n_1} + \sfrac{1}{n_2}}} \thicksim N(0,1)\]
Hence if the $H_0 \ : \ \mu_X = \mu_Y$ then:
\[\cfrac{\overline{X} - \overline{Y}}{\sigma \sqrt{\sfrac{1}{n_1} + \sfrac{1}{n_2}}}\thicksim N(0,1)\]
To get an estimate for the variance we can use the \keyword{Bias-Corrected Pooled Sample Variance}

\begin{examplebox}{Compiler Comparison}
	Given two compilers, attempt to determine if compiler 2 produces is faster code (to $5\%$ significance).
	\[\begin{matrix}
			\text{\textbf{Compiler 1}} & \text{\textbf{Compiler 2}} \\
			n_1 = 15                   & n_2 = 15                   \\
			\overline{x} = 114s        & \overline{y} = 94s         \\
			s_{14}^2 = 310             & s_{14}^2 = 290             \\
			\mu_1                      & \mu_2                      \\
		\end{matrix}\]
	\[H_0 \ : \ \mu_1 \leq \mu_2 \ \text{  versus  } \ H_1 \ : \ \mu_1 > \mu_2\]
	We assume that the variances of the population variances are the same for both compilers.
	\\
	\\ We can get the \keyword{Bias-Corrected Pooled Sample Variance}:
	\[S_{28} = \cfrac{14 \times 310 + 14 \times 290}{14 + 14} = 300\]
	Hence our \keyword{test statistic} is:
	\[\cfrac{\overline{x} - \overline{y}}{\sigma \sqrt{\sfrac{1}{n_1} + \sfrac{1}{n_2}}} = \cfrac{20}{\sqrt{300}\sqrt{\cfrac{2}{15}}} = \sqrt{10} \approx 3.162\]
	We can now use the \keyword{student's t distribution} to get the rejection region (one-sided):
	\[t_{28, 0.95} = 1.701\]
	Hence as $3.162 > 1.701$ we have sufficient evidence at the $5\%$ significance to reject $H_0$ and accept $H_1$. The second compiler produces faster code.
\end{examplebox}


\begin{sidenotebox}{Welch's t-test}
	If the variances are unknwon, and not equal, we can use \keyword{Welch's t test}.
	\\
	\\ The \keyword{test statistic} is:
	\[\cfrac{(\overline{x} - \overline{y}) - (\mu_X - \mu_Y)}{\sqrt{\cfrac{S_{n_1, X}^2}{n_1} + \cfrac{S_{n_1, Y}^2}{n_1}}}\]
	We then use a t distribution $t_\nu$ with the $\nu$ degrees of freedom determined by rounding the following to the nearest whole number:
	\[\nu = \cfrac{\left(\left(\cfrac{S^2_{n_1, \ X}}{n_1}\right)+ \left(\cfrac{S^2_{n_1, \ X}}{n_1}\right) \right)^2}{\left(\cfrac{1}{n_1 - 1}\right)\left(\cfrac{S^2_{n_1, \ X}}{n_1}\right)^2 + \left(\cfrac{1}{n_2 - 1}\right)\left(\cfrac{S^2_{n_2, \ Y}}{n_2}\right)^2}\]
	The we proceed as normal, checking the test statistic is within the rejection regions.
\end{sidenotebox}

\section{Chi Squared Testing}

\subsection{Goodness of Fit}
\begin{definitionbox}{Binning}
	Given a distribution, we can partition it into several disjoint \keyword{bins}. Essentially we are creating a pesudo-\keyword{PMF} (potentially with ranges instead of just discrete values) describing how many datapoints/the frequency we would expect to find from a distribution.
	\\
	\\ As a result, we can directly compare the expected values $E_i$ (from a distribution we are checking a sample against), with the observations $O_i$ from a sample.
	\centerimage{width=0.75\textwidth}{hypothesis_testing/images/binning.drawio.png}
\end{definitionbox}
\begin{definitionbox}{Goodness of Fit/Chi-Square Statistic }
	Denotes the difference between some expected values, and some observed.
	\\
	\\ For $n$ bins we have:
	\[X^2 = \sum_{i=1}^n\cfrac{(O_i-E_i)^2}{E_i}\]
\end{definitionbox}
\subsection{Chi-Squared Test for Model Checking}

Used to determine if an observed sample matches a given distribution to some significance.
\begin{enumerate}
	\item Determine expected distribution (can use parameters estimated from the sample).
	\item Create a hypotheses based some parameters $\theta$: {
	      \[H_0 \ : \ \theta = \theta_0 \ \text{  versus  } \ H_1 \ : \ \theta \neq \theta_0\]
	      }
	\item Bin the expected distribution (for comparison with the observed).
	\item Calculate the \keyword{Goodness of Fit/Chi-Square Test Statistic} $X^2$.
	\item Calculate the degrees of freedom as: {
	      \[\nu = (\text{number of possible values $X$ can take}) - (\text{number of parameters being estimated}) - 1\]}
	\item Determine the critical value using the \keyword{Chi Squared Distribution} $\chi^2_\nu$ and the significance $\alpha$ (typically using a table).
	\item If $X^2 > \chi^2_{\nu, \ 1 - \alpha}$ (test statistic larger than critical value)
\end{enumerate}
Note that:
\begin{itemize}
	\item All expected values must be larger than $5$ for a good test. Hence some bins may have to be merged.
	\item The number of values $X$ can take is typically the number of bins.
\end{itemize}

\centerimage{width=0.6\textwidth}{hypothesis_testing/images/chi_squared.drawio.png}

\begin{examplebox}{Adverse Drug Effects}
A study in the journal of the American Medical Association gives the causes of a sample of $95$ adverse drug effects as:
\begin{center}
	\begin{tabular}{l l}
		\textbf{Reason}   & \textbf{No. Adverse Effects} \\
		\hline
		Lack of Knowledge & $29$                         \\
		Rule Violation    & $17$                         \\
		Faulty Dose Check & $13$                         \\
		Slips             & $9$                          \\
		Other Cause       & $27$                         \\
	\end{tabular}
\end{center}
Test if the true percentages of causes of adverse effects are different at the $5\%$ significance.
\\
\\ As we are checking the percentages are the same, we effectively have a discrete uniform distribution:
\[X \thicksim U(1,5)\]
Hence we can calculate our \keyword{null and alternative hypotheses}:
\[H_0 \ : \ X \thicksim U(1,5) \ \text{  versus  } \ H_1 \ : \ X \not\thicksim U(1,5)\]
Now we can bin the distribution, (no merging is required as all expected values are larger than $5$):
\centerimage{width=0.6\textwidth}{hypothesis_testing/images/ade_example.drawio.png}
It is now possible to compute goodness of fit.
\[\begin{split}
		X^2 &= \sum_{i=1}^n\cfrac{(O_i-E_i)^2}{E_i} \\
		&= \cfrac{(29 - 19)^2}{19} + \cfrac{(17 - 19)^2}{19} + \cfrac{(13 - 19)^2}{19} + \cfrac{(9 - 19)^2}{19} + \cfrac{(27 - 19)^2}{19} \\
		&= 16 \\
	\end{split}\]
We have $\nu = 4$ as there are $5$ possible values, and no parameters were estimated from the data.
\\
\\ Hence we get the critical value from the chi-squared table: $\chi^2_{4, \ 0.95} = 9.49$
\\
\\ As $16 > 9.49$ there is sufficient evidence at the $5\%$ significance level to reject $H_0$, the percentages differ.
\end{examplebox}
\begin{examplebox}{Football Games}
Given the total number of goals for $2608$ football matches, determine if the number of goals scored in a match can be modelled by $X \thicksim Poisson(3.870)$ at the $5\%$ significance.
\begin{center}
	\begin{tabular}{l | c c c c c c c c c c c c}
		Goals Scored ($x$) & 0  & 1   & 2   & 3   & 4   & 5   & 6   & 7   & 8   & 9  & $\geq 10$      \\
		\hline
		Matches ($n_x$)    & 57 & 203 & 383 & 525 & 532 & 408 & 273 & 139 & 139 & 45 & 27        & 16 \\
	\end{tabular}
\end{center}
Hence as we already have a distribution, we can create our hypotheses:
\[H_0 \ : \ X \thicksim Poisson(3.870) \ \text{  versus  } \ H_1 \ : \ X \not\thicksim Poisson(3.87)\]
We can then use the poisson distribution to calculate the expected for $2608$ football matches, for the final ($\geq 10$) we use the cumulative to get the remaining probability.
\begin{center}
	\begin{tabular}{l | c c c c c c c c c c c c}
		Goals                  & 0     & 1     & 2     & 3     & 4     & 5     & 6     & 7     & 8     & 9     & $\geq 10$ \\
		\hline
		$O$                    & 57    & 203   & 383   & 525   & 532   & 408   & 273   & 139   & 45    & 27    & 16        \\
		$E$                    & 54.4  & 210.5 & 407.4 & 525.5 & 508.4 & 393.5 & 253.8 & 140.3 & 67.9  & 29.2  & 17.1      \\
		$\cfrac{(O - E)^2}{E}$ & 0.124 & 0.267 & 1.461 & 0.000 & 1.096 & 0.534 & 1.452 & 0.012 & 7.723 & 0.166 & 0.071     \\
	\end{tabular}
\end{center}
Hence we get our test statistic as: $X^2 = 12.906$.
\\
\\ As we did not estimate any parameters from the sample, the degrees of freedom are $\nu = 11 - 1 = 10$.
\\
\\ The critical value is: $\chi^2_{10, \ 0.95} = 16.91$.
\\
\\ Hence as $12.906 < 16.91$ we there is insufficient evidence as the $5\%$ significance to reject $H_0$, the goals can be modelled as $Poisson(3.87)$.
\end{examplebox}
\subsection{Chi-Squared Test for Independence}
\begin{definitionbox}{Contingency Table}
	A table denoting the frequency of each combination of values for $X$ and $Y$.
	\begin{center}
		\begin{tabular}{l c | c c c c | c}
			                              &          & \multicolumn{4}{c|}{Possible values of $y$} & Marginal                                                          \\
			                              &          & $y_1$                                       & $y_2$            & $\dots$  & $y_l$            &                  \\
			\hline
			\multirow{4}{*}{Possible $x$} & $x_1$    & $n_{1,1}$                                   & $n_{1,2}$        & $\dots$  & $n_{1,l}$        & $n_{1, \bullet}$ \\
			                              & $x_2$    & $n_{2,1}$                                   & $n_{2,2}$        & $\dots$  & $n_{2,l}$        & $n_{2, \bullet}$ \\
			                              & $\vdots$ & $\vdots$                                    & $\vdots$         & $\ddots$ & $\vdots$         & $\vdots$         \\
			                              & $x_k$    & $n_{k,1}$                                   & $n_{k,2}$        & $\dots$  & $n_{k,l}$        & $n_{k, \bullet}$ \\
			\hline
			Marginal                      &          & $n_{\bullet, 1}$                            & $n_{\bullet, 2}$ & $\dots$  & $n_{\bullet, l}$ & $n$              \\
		\end{tabular}
	\end{center}
	We can use the marginal values to determine the expected value, if the two distributions were independent.
\end{definitionbox}
Given a dataset of points $(x,y)_1, (x,y)_2, \dots, (x,y)_n$, we can consider it the joint distribution $P_{XY}$ of the distributions $P_X$ and $P_Y$.
\\
\\ To test if the distributions $P_X$ and $P_Y$ are independent from the sample (without knowing the actual distributions themselves) we can use a \keyword{contingency table}.
\\
\\ For the contingency table entry coordinates $0 < i \leq l$, $0 < j \leq k$:
\[O_{i,j} = n_{i,j} \ \text{ and } E_{i,j} = \cfrac{n_{i, \bullet} \times n_{\bullet, j}}{n}\]
Hence we can now compute the $X^2$ (\keyword{Chi Squared test statistic}) using these observed and expected values.
\\
\\ We compute the degrees of freedom as $\nu = (rows - 1) \times (columns - 1)$ (each row and column alone has degrees of freedom $n-1$ as they must sum to the row/column total), and can then do the \keyword{Chi-Squared Test} normally.
\begin{examplebox}{Fitness and Stress}
\begin{center}
	\begin{tabular}{l | c c c | c}
		          & Poor Fitness & Average Fitness & Good Fitness &     \\
		\hline
		Stress    & 206          & 184             & 85           & 475 \\
		No Stress & 36           & 28              & 10           & 74  \\
		\hline
		          & 242          & 212             & 95           & 549 \\
	\end{tabular}
\end{center}
Determine at the $5\%$ significance if there is a link between fitness and stress.
\\
\\ For this test the null hypothesis will be that fitness and stress are independent.
\[H_0 \ : \ \text{Stress and fitness are independent} \ \text{  versus  } \ H_1 \ : \ \text{Stress and Fitness re not independent}\]
Next we can calculate the expected values:
\begin{center}
	\begin{tabular}{l | c c c c c c | c}
		          & \multicolumn{2}{c}{Poor Fitness} & \multicolumn{2}{c}{Average Fitness} & \multicolumn{2}{c}{Good Fitness} &                                                                             \\
		          & $O$                              & $E$                                 & $O$                              & $E$                    & $O$                  & $E$                   &     \\
		\hline
		Stress    & \textcolor{blue}{206}            & \textcolor{red}{209.4}              & \textcolor{blue}{184}            & \textcolor{red}{183.4} & \textcolor{blue}{85} & \textcolor{red}{82.2} & 475 \\
		No Stress & \textcolor{blue}{36}             & \textcolor{red}{32.6}               & \textcolor{blue}{28}             & \textcolor{red}{28.6}  & \textcolor{blue}{10} & \textcolor{red}{12.8} & 74  \\
		\hline
		          & \multicolumn{2}{c}{242}          & \multicolumn{2}{c}{212}             & \multicolumn{2}{c}{95}           & 549                                                                         \\
	\end{tabular}
\end{center}
We can then calculate our test statistic to be $X^2 = 1.133$.
\\
\\ To compute the degrees of freedom $\nu = (2 - 1) \times (3 - 1) = 2$.
\\
\\ Hence we can get our critical value $\chi^2_{2, \ 0.95} = 5.99$.
\\
\\ As $5.99 > 1.133$, there is insufficient evidence to reject $H_0$ at the $5\%$ significance level. Stress and fitness are not linked.
\end{examplebox}


