\chapter{Statistics and Estimation}

\section{Statistics Terms}
\begin{definitionbox}{Probability}
	\centerimage{width=0.6\textwidth}{statistics_and_estimation/images/probability.drawio.png}
	Deducing likelihood, and predicting events based on a known probability distribution.
\end{definitionbox}
\begin{definitionbox}{Statistics}
	\centerimage{width=0.6\textwidth}{statistics_and_estimation/images/statistics.drawio.png}
	Using empirical data/observations from an experiment to determine a probability distribution (and estimate its parameters) that models the observed distribution of results.
\end{definitionbox}
\begin{definitionbox}{Sample}
	\centerimage{width=0.6\textwidth}{statistics_and_estimation/images/sample.drawio.png}
	A subset of the population, from which we can use \keyword{statistical methods} to make inferences about the characteristics of an entire population.
	\begin{itemize}
		\item In vaccine trials, we can take a random sample as participants, and use there results to infer the possible efficacy of the vaccine over an entire population.
		\item In manufacturing we may want to test durability, but doing so may destroy the product. Hence we can take a small representative sample, and tests these to gain knowledge about the durability of all products from a given production line, without having to test all to destruction.
		\item In politics, we can use the political persuasions of a sample to reason about an entire population (such as electorate, or a given group) (polling).
	\end{itemize}
\end{definitionbox}
\begin{definitionbox}{Statistical Models}
	Models are a structure (e.g distribution) often developed from a sample that can be used to make inferences about a population.
	\begin{itemize}
		\item Models are usually \keyword{parametric}, meaning the models can be described entirely by its parameters.
		\item Models have a finite set of parameters.
	\end{itemize}
	\centerimage{width=0.8\textwidth}{statistics_and_estimation/images/model.drawio.png}
	\begin{itemize}
		\item We can use distributions such as \keyword{Normal}, \keyword{Poisson}, \keyword{Bernoulli} etc. as parametric models.
		\item If the population is such that the probability of each outcome is $P_{X|\theta}(.|\theta)$ (probability of each is only dependent on parameters) we can assume the random variables $\underline{X}$ are independent and identically distributed.
		\item $X_1, X_2, \dots, X_n \thicksim Model(\theta_1, \theta_2, \dots, \theta_k)$ given all are identically \& independently distributed.
	\end{itemize}
\end{definitionbox}
\section{Central Limit Theorem for Statistics}
\begin{definitionbox}{Central Limit Theorem}
	Given some distribution random variable $X$ belonging to some distribution. The mean value of a sample of size $n$ from $X$ is:
	\[Y \thicksim N(\mu, \cfrac{\sigma^2}{n})\]
	Where $\mu$ is the expected/mean value of $X$ and $\sigma^2$ is its variance.
	\\
	\\ As the sample size increases, the variance in mean between different samples reduces.
	\\
	\\ At an infinite sample size, we can use the \keyword{standard normal distribution}:
	\[\lim_{n \to \infty} \left( \cfrac{Y - \mu}{\cfrac{\sigma}{\sqrt{n}}} \right) \thicksim N(0,1)\]
\end{definitionbox}
\begin{examplebox}{Ages of a class}
	Given a class of $20$ students, we can calculate the mean and variance:
	\[\overline{x} = \cfrac{1}{20} \sum_{i = 1}^{20}x_i \ \text{ and } \ \overline{\sigma}^2 = \cfrac{1}{20} \sum_{i = 1}^{20} (x_i - \overline{x})^2\]
	There is some unknown distribution of students ages in a class.
	\\
	\\ If sampling is done with replacement (not students removed from the population after being questioned) we can use the central limit theorem to model the mean and variance of this distribution's mean (the mean age of the class) without needing to know the distribution itself.
	\[\overline{x} \ \text{is distributed according to } \ N(\mu, \cfrac{\sigma^2}{20})\]
	Meaning  the mean age of any group of $20$ students will be distributed normally with parameters:
	\begin{itemize}
		\item $\mu$ (The average age of all students/ avergae of all possible groups of 20)
		\item $\sigma^2$ (The variance of means, how different two groups of $20$ stuident's means may be expected to be).
    \end{itemize}
	As we increase sample size, the variance decreases (larger groups of student $\Rightarrow$ means closer together).
	\\
	\\ We will use this later in tests, e.g to see if a given mean that occurs is so unlikely it is likely our distribution is wrong, or our sampling biased in some way.
\end{examplebox}
\section{Estimators}
\begin{definitionbox}{Statistic}
	A \keyword{statistic} is a function operating on the random variables of a sample:
	\[T = T(X_1, X_2, \dots, X_n) = T(\underline{X})\]
	As it is a function of random variables, it is itself a random variable. Hence if distribution $X$'s parameters are known, we can use it:
	\begin{itemize}
		\item if $T$ is the sum of ages of a class of $10$, and we know the mean age, variance we can calculate porbabilities for $T$.
		\item $T$ may be many useful statistics, e.g the lower quartile of a cohort of $100$'s GCSE results, or the range of distances flown by birds in a flock.
	\end{itemize}
	When given some sample $\underline{x} = (x_1, x_2, \dots, x_n)$ we have:
	\[t = t(\underline{x}) = t(x_1, x_2, \dots, x_n)\]
\end{definitionbox}
\begin{definitionbox}{Estimator}
	A statistic used to approximate the parameter of the distribution of its arguments.
	\begin{itemize}
		\item Given a sample $\underline{x}$ the value of the estimator $t = t(\underline{x})$ is called an estimate.
		\item If we can approximately identify the sampling distribution of the statistic ($P_{T | \theta}$) we can find the expectation, variance (and more) related to our statistic.
	\end{itemize}
	If the sample size $n$ is large, \keyword{central limit theorem} can be used to approximate the distribution $P_{T|\theta}$
	\[T = \overline{X} = \cfrac{\sum_{i=1}^{n}X_i}{n}\]
	And hence we know approximately that:
	\[\overline{X} \thicksim N(\mu_X, \cfrac{\sigma_X^2}{n})\]
\end{definitionbox}

For a given unknown distribution we could use several estimators to approximate its parameter.
\subsubsection*{Using the first/any $X_i$ as the estimator}
\[T[X_1, X_2, \dots , X_n] = X_1 \thicksim P_{X | \theta}\]
Likewise if we use the median with $T$:
\[T_{median} [X_1, X_2, \dots, X_n] = X_{\left| \cfrac{n+1}{2} \right|} \thicksim P_{X | \theta} \]
However this does not work as we do not know the parameters of the distribution $X$.
\subsubsection*{Using the mean as an estimator}
\[T_{\overline{X}}[X_1, X_2, \dots , X_n] = \cfrac{\sum_{i = 1}^nX_i}{n} \thicksim N(\mu, \cfrac{\sigma^2}{n})\]
This is a good estimator for the mean of many distributions, while we do not know $\mu$ or $\sigma$, we do know the type of distribution.

\begin{definitionbox}{Estimator Bias}
We define the bias of an estimator $T$ as estimating the parameter $\theta$ is:
\[bias(T) = E[T | \theta] - \theta\]
If bias is $0$ we call it an unbiased estimator.
\\
\\ \textbf{For the mean:}
\[E(\overline{X}) = E \left( \cfrac{\sum_{i=1}^nX_i}{n} \right) = \cfrac{\sum_{i=1}^nE[X_i]}{n} = \cfrac{n \times \mu}{n} = \mu\]
For any distribution the sample mean $\overline{x}$ is an unbiased estimate for the population mean $\mu$.
\\
\\ \textbf{For the variance:}
If we know the population mean $\mu$ we can also use the unbiased estimator:
\[S_\mu^2 = \cfrac{1}{n}\sum_{i=1}^n(X_i - \mu)^2\]
The sample variance is a biased estimator and is defined as:
\[S^2 = \cfrac{1}{n}\sum_{i=1}^n(X_i - \overline{X})^2\]
We have too few degrees of freedom, that is based on the mean and $x_{1 \to n-1}$ we can determine $x_n$, hence we apply \keyword{bessel's correction} (wikipedia article on source of bias \href{https://en.wikipedia.org/wiki/Bessel\%27s_correction}{here}) to account for what is effectively a missing variance.
\\
\\ After applying bessel's correction, we get the unbiased estimator of \keyword{bias-corrected sample variance}:
\[S_{n-1}^2 = \cfrac{1}{n - 1}\sum_{i=1}^n(X_i - \overline{X})^2\]
\end{definitionbox}

\subsection{Bessel's Correction Proof}
First we attempt to prove that $S_\mu^2$ is an unbiased estimator for variance.
\\
\\ \textbf{1.} We first define $S_\mu^2$.
\[S_\mu^2 = \cfrac{1}{n}\sum_{i=1}^n(X_i - \mu)^2\]
\textbf{2. } We get the expected value of the estimator, to be an unbiased estimator of variance, this should be equal to the variance.
\[\begin{split}
		E[S_\mu^2] &= E\left[ \cfrac{1}{n} \sum_{i = 1}^n \left( X_i - \mu \right)^2 \right] \\
		&= \cfrac{1}{n}\sum_{i = 1}^n E\left[X_i^2 - 2X_i\mu + \mu^2 \right] \\
		&= \cfrac{1}{n}\sum_{i = 1}^n \left( E[X_i^2] - 2E[X_i]\mu + \mu^2 \right) \\
	\end{split}\]
\textbf{3. } We can substitute $\mu$ for $E[X_i]$:
\[\begin{split}
		E[S_\mu^2] &= \cfrac{1}{n}\sum_{i = 1}^n \left( E[X_i^2] - 2E[X_i]E[X_i] + (E[x_i])^2 \right) \\
		&= \cfrac{1}{n}\sum_{i = 1}^n \left( E[X_i^2] - (E[x_i])^2 \right) \\
		&= \cfrac{1}{n}\sum_{i = 1}^n Var[X_i] \\
	\end{split} \]
\textbf{4. } As all $X_i$ are identically distributed, $Var[X_i] = Var[X] = \sigma^2$.
\[\begin{split}
		E[S_\mu^2] &= \cfrac{1}{n}\sum_{i = 1}^n \sigma^2 \\
		&= \cfrac{n \times \sigma^2}{n} \\
		&= \sigma^2 \\
	\end{split}\]
Hence we can see that $S^2_\mu$ is an unbiased estimator of $\sigma^2$.
\\
\\ Next we prove the correction:
\\ \textbf{1. } We get the expected of:
\[E\left[ \sum_{i = 1}^n ( X_i - \overline{x} )^2 \right]\]
\textbf{2. } We can add and subtract $\mu$ (keeping the same value)
\[E\left[ \sum_{i = 1}^n ( X_i - \overline{x} )^2 \right] = E\left[ \sum_{i = 1}^n ( (X_i - \mu) - (\overline{x} - \mu) )^2 \right]\]
\textbf{3. } Now we can split the expected up (all distributions are independent (the normal for $\overline{x}$ and we assume independence for $X_i$)).
\[E\left[ \sum_{i = 1}^n ( X_i - \overline{x} )^2 \right] = E\left[ \left( \sum_{i = 1}^n (X_i - \mu)^2 \right) - 2(\overline{x} - \mu)\left( \sum_{i = 1}^n (X_i - \mu) \right) + \left( \sum_{i = 1}^n(\overline{x} - \mu)^2 \right) \right]\]
\textbf{4. } We can substitute using $\sum_{i = 1}^n (X_i - \mu) = n \times (\overline{x} - \mu)$.
\[\begin{split}
		E\left[ \sum_{i = 1}^n ( X_i - \overline{x} )^2 \right] &= E\left[ \left( \sum_{i = 1}^n (X_i - \mu)^2 \right) - 2(\overline{x} - \mu) \times n \times (\overline{x} - \mu) + \left( \sum_{i = 1}^n(\overline{x} - \mu)^2 \right) \right] \\
		&= E\left[ \left( \sum_{i = 1}^n (X_i - \mu)^2 \right) - 2n(\overline{x} - \mu)^2 + \left( \sum_{i = 1}^n(\overline{x} - \mu)^2 \right) \right] \\
		&= E\left[ \left( \sum_{i = 1}^n (X_i - \mu)^2 \right) - 2n(\overline{x} - \mu)^2 + n(\overline{x} - \mu)^2 \right] \\
		&= E\left[ \left( \sum_{i = 1}^n (X_i - \mu)^2 \right) - n(\overline{x} - \mu)^2 \right] \\
	\end{split}\]
\textbf{5. } We can split the expected (independent distributions) substitute in the variance $X$.
\[\begin{split}
		E\left[ \sum_{i = 1}^n ( X_i - \overline{x} )^2 \right] &= E\left[ \left( \sum_{i = 1}^n (X_i - \mu)^2 \right) - n(\overline{x} - \mu)^2 \right] \\
		&= E \left[ \sum_{i = 1}^n (X_i - \mu)^2 \right] - n \times E\left[ \left( \overline{x} - \mu \right)^2 \right]\\
		&= \sum_{i = 1}^n E\left[ (X_i - \mu)^2 \right] - n \times E\left[ \left( \overline{x} - \mu \right)^2 \right]\\
	\end{split}\]
\textbf{5. } As $\overline{x}$ is distributed by a normal distribution $N(\mu, \cfrac{\sigma^2}{n})$, the expected of it shifted by $\mu$ and squared is the variance.
\[\begin{split}
		E\left[ \sum_{i = 1}^n ( X_i - \overline{x} )^2 \right] &= \sum_{i = 1}^n E\left[ (X_i - \mu)^2 \right] - n \times \cfrac{\sigma^2}{n}\\
		&= \sum_{i = 1}^n E\left[ (X_i - \mu)^2 \right] - \sigma^2\\
	\end{split}\]
\textbf{6. } We can then use the variance of the distribution of $X$:
\[\begin{split}
		E\left[ \sum_{i = 1}^n ( X_i - \overline{x} )^2 \right] &= \sum_{i = 1}^n E\left[ (X_i - \mu)^2 \right] - \sigma^2\\
		&= n \sigma^2 - \sigma^2\\
		&= (n - 1) \sigma^2\\
	\end{split}\]
\textbf{7. } Hence to get an unbiased estimator, we need to divide this by $(n-1)$ (apply correction).
\[\begin{split}
		E\left[ \sum_{i = 1}^n ( X_i - \overline{x} )^2 \right] &= (n - 1) \sigma^2\\
		\cfrac{1}{n-1} E\left[ \sum_{i = 1}^n ( X_i - \overline{x} )^2 \right] &= \sigma^2 \\
		E\left[\cfrac{1}{n-1} \sum_{i = 1}^n ( X_i - \overline{x} )^2 \right] &= \sigma^2 \\
	\end{split}\]
Hence $\cfrac{1}{n-1} \sum_{i = 1}^n ( X_i - \overline{x} )^2 $ is an unbiased estimator of $\sigma^2$.

\section{Efficient Consistent Estimator}
We can quantify how \textit{good} estimators are. For example with the \keyword{Estimator Bias} (difference
between the expected using the estimator and the parameter $bias(T) = E[T|\theta] - \theta$). We also wanto to
quantify the \keyword{Efficiency of Estimators}.
\begin{definitionbox}{Estimator Efficiency}
	Given two unbiased estimators $\hat{\Theta}(\underline{X})$ and $\tilde{\Theta}(\underline{X})$ where $\underline{X} = (X_1, \dots, X_n)$ (a sample containing $n$ observations $X\dots$).
	\\
	\\ We can compare the mean, variances etc to determine which estimator is more efficient (typically lower variance)
	\\
	\\ $\hat{\Theta}$ is more efficient than $\tilde{\Theta}$ if:
	\[\forall \theta Var_{\hat{\Theta}}(\hat{\Theta}| \theta) \leq Var_{\tilde{\Theta} | \theta}(\tilde{\Theta} | \theta) \
		\ \text{ or }
		\ \
		\exists \theta Var_{\hat{\Theta}}(\hat{\Theta}| \theta) < Var_{\tilde{\Theta} | \theta}(\tilde{\Theta} | \theta)
	\]
	More efficient means less variance in estimates.
	\\
	\\ IF an estimator is more efficient than any other possible estimator, it is called \keyword{efficient}.
\end{definitionbox}
\begin{examplebox}{Bias and Efficiency}
	Given a population with mean $\mu$ and variance $\sigma^2$. We have a sample:
	\[\underline{X} = (X_1, \dots, X_n)\]
	We consider two estimators:
	\begin{enumerate}
		\item $\hat{M} = \overline{X}$ (the sample mean)
		\item $\tilde{M} = X_1$ (the first observation in the sample)
    \end{enumerate}
	We can compute the bias as for both:
	\begin{enumerate}
		\item The expected value of the sample mean is the population mean $\mu$, hence $\hat{M}$ is unbiased.
		\item The expected value of any observation is $\mu$, so the first observation in the sample is also ubiased.
	\end{enumerate}
	Next we can consider the variance.
	\\
	\\ For a single sample we know the variance will be $\sigma^2$, hence:
	\[Var_{\tilde{M}}(\tilde{M}|\mu \text{ and } \sigma^2) = Var(X_1) = \sigma^2\]
	However for the sample mean, we know can use the \keyword{Central Limit Theorem} to determine that the variance of the mean of a sample will be divided by the sample size.
	\[Var_{\hat{M}}(\hat{M}|\mu \text{ and } \sigma^2) = Var(\overline{X}) = \cfrac{\sigma^2}{n}\]
	Hence for all values of $n$, the variance of $\hat{M} \leq \tilde{M}$ (at $n=1$ they are equal), so $\hat{M}$ is the more efficient estimator.
\end{examplebox}
\begin{definitionbox}{Estimator Consistency}
	A consistent estimator improves as the sample size grows. Formally:
	\[\forall \epsilon > 0 \ P(|\hat{\Theta} - \theta|) \to 0 \ \text{ as } \ n \to \infty\]
	If $\hat{\Theta}$ is unbiased, then:
	\[\lim_{n \to \infty} Var(\hat{\Theta}) = 0 \Rightarrow \hat{\Theta} \ \text{ is consistent}\]
	Note: $\overline{X}$ (sample mean) is a consistent estimator for any population.
\end{definitionbox}
\section{Confidence Intervals}
\centerimage{width=0.75\textwidth}{statistics_and_estimation/images/estimate.drawio.png}
In order to quantify our degree of uncertainty in an estimate $\hat{\theta}$, when the true value $\theta$ is unknown, we use use our estimate as the true value, to compute the distribution $P_{T|\hat{\theta}}$ (the approximate sampling distribution).
\subsection{Known Variance}
\subsubsection{Confidence Interval}
If we know the true variance of the population, then the sample mean would be distributed as:
\[\overline{X} \thicksim N\left(\overline{x}, \cfrac{\sigma^2}{n} \right)\]
If $\mu$ (population mean) $= \overline{x}$, then we can say that (using the standard normal distribution) there is a $95\%$ probability the observed statistic $\overline{X}$ is in the range:
\[\left[\overline{x} - 1.96\cfrac{\sigma}{n}, \overline{x} + 1.96\cfrac{\sigma}{n} \right]\]
(Double ended, $95\%$ confidence interval for $\mu$)
\centerimage{width=0.6\textwidth}{statistics_and_estimation/images/confidence_interval.drawio.png}
\subsubsection{With the Standard Normal Distribution}
We can define any normal distribution in terms of the standard normal distribution.
\[X \thicksim N(\mu, \sigma^2) \Leftrightarrow Y = \cfrac{X - \mu}{\sigma} \Leftrightarrow Y \thicksim N(0,1)\]
We can then use tables for the standard normal distribution, using $\Phi(z) = P(X \leq z)$ given $Z \in N(0,1)$:
\\
\\ Note if you have sample size as part of the variance, $Y = \cfrac{X - \mu}{\left(\cfrac{\sigma}{\sqrt{n}}\right)}$.
\\
\\ For example in the previous confidence interval, we used the normal distribution to calculate the values.
\centerimage{width=0.6\textwidth}{statistics_and_estimation/images/confidence_standard_normal.drawio.png}
Given the critical value $z$ for the normal distribution e.g $1.96$ for double-ended $95\%$ confidence interval, we have:
\begin{center}
	\begin{tabular}{r c c}
		Standard Normal     & $X \thicksim N(0,1)$                                            & $[-z, z]$                                                                                         \\
		Normal Distribution & $X \thicksim N(\mu, \sigma^2)$                                  & $\mu - z\sigma, \mu + z\sigma$                                                                    \\
		Sample Mean         & $\overline{X} \thicksim N\left(\mu, \cfrac{\sigma^2}{n}\right)$ & $\left[\mu - z\cfrac{\sigma}{\sqrt{n}}, \mu + z\cfrac{\sigma}{\sqrt{n}}\right]$                   \\
		\\
		Population mean     & $\mu \thicksim N\left(\overline{X}, \cfrac{\sigma^2}{n}\right)$ & $\left[\overline{x} - z\cfrac{\sigma}{\sqrt{n}}, \overline{x} + z\cfrac{\sigma}{\sqrt{n}}\right]$ \\
	\end{tabular}
\end{center}
\begin{examplebox}{Employees Opinions on the Board}
	A corporation surveys employees on wether they think the board is doing a good job.
	\\
	\\ $1000$ employees are randomly selected, and $732$ say the board is doing a good job. Find the $99\%$
	confidence interval for the proportion of the employees that think the board is doing a good job. Assume the variance is $\sigma^2 = 0.25$.
	\\
	\\ First we get the sample mean:
	\[\overline{x} = \cfrac{732}{1000} = 0.732\]
	Next we determine the standard deviation:
	\[\sigma = \sqrt{0.25} = 0.5\]
	We want to get the double-ended $99\%$ interval, so each tail will have size $0.005$. By using the standard normal distribution we have $\Phi(2.576) = 0.995$, so $z = 2.576$.
	\\
	\\ Hence we can calculate the interval as:
	\[\begin{split}
			\mu &= \left[\overline{x} - z\cfrac{\sigma}{\sqrt{n}}, \overline{x} + z\cfrac{\sigma}{\sqrt{n}}\right] \\
			&= \left[0.732 - 2.576\cfrac{0.5}{\sqrt{1000}}, 0.732 + 2.576\cfrac{0.5}{\sqrt{1000}}\right] \\
			&= \left[0.732 - 2.576\cfrac{0.5}{\sqrt{1000}}, 0.732 + 2.576\cfrac{0.5}{\sqrt{1000}}\right] \\
			&\approx 0.732 \pm 0.0407 \\
		\end{split}\]
\end{examplebox}

\subsection{Unknown Variance}
In a problem where we are trying to fit a normal distribution, but both the mean and variance are unknown.
\[\text{Bias Corrected Variance} \ S_{n-1} = \sqrt{\cfrac{\sum_{i=1}^{n}(X_i - \overline{X})^2}{n-1}}\]
We use the bias corrected variance of our sample, and as a result must use a different distribution to the normal distribution.
\begin{center}
	\begin{tabular}{c | c}
		\textbf{Normal Distribution ($\sigma$ known)}                                        & \textbf{Studen't t distribution ($\sigma$ unknown)}                                    \\
		\\
		$\cfrac{\overline{X} - \mu}{\left(\cfrac{\sigma}{\sqrt{n}}\right)} \thicksim N(0,1)$ & $\cfrac{\overline{X} - \mu}{\left(\cfrac{s_{n-1}}{\sqrt{n}}\right)} \thicksim t_{n-1}$ \\
	\end{tabular}
\end{center}
In the student's distribution we set degrees of freedom $\nu = n - 1$.
\\
\\ For a double ended confidence $(100 - \alpha)\%$, we compute $t_{\nu = n - 1, \ 1 - \sfrac{\alpha}{2}}$ to find the critical values (the places where the tails start/ the $\alpha$-quantile of $t_\nu$).
\centerimage{width=0.6\textwidth}{statistics_and_estimation/images/students_distribution.drawio.png}
\[\left[ \overline{x} - t_{\nu = n - 1, \ 1 - \sfrac{\alpha}{2}} \times \cfrac{s_{n-1}}{\sqrt{n}}, \ \overline{x} + t_{\nu = n - 1, \ 1 - \sfrac{\alpha}{2}} \times \cfrac{s_{n-1}}{\sqrt{n}}\right]\]
When using the tables for $t$ values, we use the size we want (e.g $0.975$ for $95\%$ double-ended confidence interval), and then use the degrees of freedom ($n-1$).
