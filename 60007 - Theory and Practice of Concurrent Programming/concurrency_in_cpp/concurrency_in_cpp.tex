\definecolor{threadBlue}{RGB}{204, 229, 255}
\definecolor{threadYellow}{RGB}{255, 255, 136}
\definecolor{threadGrey}{RGB}{238, 238, 238}

\chapter{Concurrency In C++}

\section{Threads}
To interact with threads the \mintinline{Cpp}{thread} header must be included.
\begin{itemize}
	\item It provides a standard, implementation independent, interface for handling threads.
	\item Provides the \mintinline{Cpp}{std::thread} class
\end{itemize}
\begin{minted}{Cpp}
#include <thread>

namespace std {
  class thread {
  public:
    // types
    class id;
    using native_handle_type = /* implementation-defined */;

    // construct/copy/destroy
    thread() noexcept;

    // Constructor takes a function to start from, and its arguments (all type checked)
    template<class F, class... Args> explicit thread(F&& f, Args&&... args);

    // Destructor (terminates current thread if the thread has not been joined)
    ~thread();

    // Attempting to copy a thread is not allowed. Hence delete ensures no compile.
    thread(const thread&) = delete;

    // Can create thread from a thread r-value (copy)
    thread(thread&&) noexcept;

    // Attempting to copy a thread (via an immutable reference). 
    // This is not allowed, so if this operator is used it will not compile.
    thread& operator=(const thread&) = delete;
    
    // Assign a thread from an (r value - e.g expression, literal) reference
    thread& operator=(thread&&) noexcept;

    // members
    void swap(thread&) noexcept;
    bool joinable() const noexcept;

    // Wait for this thread to terminate.
    void join();

    // Allow the thread to continue executing after the thread handler (this) 
    // is destroyed
    void detach();

    // Get the unique id of the thread
    id get_id() const noexcept;

    native_handle_type native_handle();

    // static members
    static unsigned int hardware_concurrency() noexcept;
  };
}
\end{minted}
\begin{definitionbox}{Lambda}
	A lambda is a small function that can be defined in an expression, capture values in its scope (and above), and be passed as a value.
	\begin{minted}{Cpp}
// [captures] (arguments) {body}

// a basic lambda with no captures
auto my_lambda = [] (int a, int b) -> int {return a + b;}

// using the lambda
int c = my_lambda(1, 2);

auto another_lambda = [c&] (int d) {return c + d;}
    \end{minted}
\end{definitionbox}
We can construct using \mintinline{Cpp}{std::thread}'s constructors.
\begin{minted}{Cpp}

// idiomatic constructor
std::thread my_thread(StartFunction, arg1, arg2, ...)

// call constructor and assign
std::thread my_thread = std::thread(StartFunction, arg1, arg2, ...)
auto my_thread = std::thread(StartFunction, arg1, arg2, ...)

// pass lambda as function
std::thread my_thread(StartFunction, arg1, arg2, ...)
\end{minted}

When passing arguments to the thread, if these are by reference, a \mintinline{Cpp}{std::ref} or \mintinline{Cpp}{std::cref} must be used.

\begin{examplebox}{Reference this!}
	Given some function \mintinline{Cpp}{static void some_func(const int& a)} create a thread to take a reference to the number $42$.
	\tcblower
	\begin{minted}{Cpp}
int a = 42;
std::thread my_thread(some_func, std::cref(42));
my_thread.join();  
  \end{minted}
\end{examplebox}

\subsection{Vectors of Threads}
When adding an object to a container (e.g a vector) we want to avoid allocating the object, and then moving it into the container.
\begin{itemize}
	\item Some objects may not be movable/copyable.
	\item The object should be allocated within the container.
\end{itemize}
For this we can use emplacement.
\begin{minted}{Cpp}
template< class... Args >
void emplace_back( Args&&... args );
\end{minted}

\begin{examplebox}{Emplace}
	Given some function \mintinline{Cpp}{static void some_func()} create 10 threads and append to the vector using \mintinline{Cpp}{std::vector::push_back} and another 10 with \mintinline{Cpp}{std::vector::emplace}.
	\tcblower
	\begin{minted}{Cpp}
std::vector<std::thread> threads;

for (int i; i < 10; i++) {
  threads.push_back(std::thread(some_func));
}

for (int i; i < 10; i++) {
  threads.emplace_back(some_func);
}

for (auto& t : threads) {
  t.join();
}    
  \end{minted}
\end{examplebox}

\subsection{This Thread}
The threads header also provides functionality for interacting with the current thread.
\begin{minted}{Cpp}
#include <compare>

namespace std {
  class thread;
  
  void swap(thread& x, thread& y) noexcept;
  
  // class jthread
  class jthread;
  
  // methods for interacting with the current thread
  namespace this_thread {
    thread::id get_id() noexcept;

    // indicates another thread should be scheduled (e.g long wait expected)
    void yield() noexcept;

    // Sleeping, generic for  
    template<class Clock, class Duration>
    void sleep_until(const chrono::time_point<Clock, Duration>& abs_time);

    //
    template<class Rep, class Period>
    void sleep_for(const chrono::duration<Rep, Period>& rel_time);
  }
}
\end{minted}
\begin{examplebox}{Clock watching}
	Create program that prints the thread id, and sleeps.
	\tcblower
	\begin{minted}{C}
#include <thread>
#include <iostream>
#include <chrono>

int main() {
  using namespace std::chrono_literals; // to use the ms syntax

  std::cout << std::this_thread::get_id() << " will sleep now!" << std::endl;
  std::this_thread::sleep_for(200ms);

  std::cout << std::this_thread::get_id() << " has awoken!" << std::endl;
}
  \end{minted}
\end{examplebox}

\section{Locks}
\begin{definitionbox}{RAII}
	\textit{Resource Acquisition Is Initialization} (also called Scope-Bound Resource Management and Constructor Acquires, Destructor Release) is where a resource's allocation and release is bound to the lifetime of an object.
	\begin{itemize}
		\item a resource may be the memory allocated to an object, or resources such as os provided file handlers.
		\item When the object goes out of scope (e.g the variable owning the object is destroyed) the resource is released.
		\item In C++, when a variable goes out of scope, the destructor of the contained object is called, so the destructor must release the resources.
		\item This concept is heavily embedded in Rust. Lifetimes are a major part of the type system, and ownership rules are enforced by the compiler.
		\item RAII is used for smart pointers such as \mintinline{Rust}{Rc} in rust or \mintinline{Cpp}{std::shared_ptr}.
	\end{itemize}
	\begin{minted}{Cpp}
static void my_scope() {
    MyClass my_object;  // initialised, default constructor called

    // ... do some stuff ...

    return; // destructor my_object.~MyClass() called. 
}
  \end{minted}
\end{definitionbox}
The \mintinline{Cpp}{mutex} header contains locks for synchronisation.
\begin{minted}{Cpp}
namespace std {
  class mutex;                  // A regular lock
  class recursive_mutex;        // reentrant/recursive lock 
  class timed_mutex;            // A mutex with timeout
  class recursive_timed_mutex;  // A recursive mutex with timeout
  
  /* used to set the locking strategy when using lock guards
   * e.g create guard (that releases lock on destruction) assuming 
   * lock is held.
   */
  struct defer_lock_t { explicit defer_lock_t() = default; };   // do not acquire ownership
  struct try_to_lock_t { explicit try_to_lock_t() = default; }; // try to acquire ownership (no block)
  struct adopt_lock_t { explicit adopt_lock_t() = default; };   // assume calling thread has ownership
  
  inline constexpr defer_lock_t  defer_lock { };
  inline constexpr try_to_lock_t try_to_lock { };
  inline constexpr adopt_lock_t  adopt_lock { };
  
  // A RAII like mechanism that releases the lock it guards when destroyed.
  template<class Mutex> class lock_guard;

  // A RAII style lock guard, when taking ownership of multiple locks it attempts 
  // deadlock avoidance.
  template<class... MutexTypes> class scoped_lock;

  // A movable lock guard.
  template<class Mutex> class unique_lock;
  
  template<class Mutex>
    void swap(unique_lock<Mutex>& x, unique_lock<Mutex>& y) noexcept;
  
  // attempts to acquire locks from references provided, returns index (in args) of lock 
  // that could not be acquired.
  template<class L1, class L2, class... L3> int try_lock(L1&, L2&, L3&...);
  
  // Acquire one or more locks (blocking) and use deadlock avoidance.
  template<class L1, class L2, class... L3> void lock(L1&, L2&, L3&...);
  
  struct once_flag;
  
  template<class Callable, class... Args>
    void call_once(once_flag& flag, Callable&& func, Args&&... args);
}
\end{minted}
\subsection{Using Mutexes}
\begin{minted}{Cpp}
namespace std {
  class mutex {
    public:

      // Constructor initialises mutex as unlocked. It is a constexpr 
      // as can determine all fields at compile time.
      constexpr mutex() noexcept;
      
      // Destructor, undefined behaviour if the mutex is held by a thread.
      ~mutex();

      // Cannot create mutex from another, or use assignment to move a mutex.
      mutex(const mutex&) = delete;
      mutex& operator=(const mutex&) = delete;
    
      void lock();
      bool try_lock();
      void unlock();
    
      using native_handle_type = /* implementation-defined */;
      native_handle_type native_handle();
  };
}
\end{minted}

\begin{examplebox}{Locked Out}
	Create a mutex to protect a counter, and use 100 threads to increment the counter 10 times each. Add a wait of 1ms between each increment and only lock for each increment.
	\tcblower
	\begin{minted}{Cpp}
#include <thread>
#include <iostream>
#include <mutex>
#include <vector>
#include <chrono>

int cnt;
std::mutex cnt_lock;

static void increment_cnt() {
  for (int i = 0; i < 10; i++) {
    std::this_thread::sleep_for(std::chrono::milliseconds(1));    
    cnt_lock.lock();
    cnt++;
    cnt_lock.unlock();
  }
}

int main() {
  std::vector<std::thread> threads;

  for (int i = 0; i < 100; i++) {
    threads.emplace_back(increment_cnt);
  }

  for (auto& t : threads) {
    t.join();
  }

  std::cout << "The counter is: " << cnt << std::endl;
}
  \end{minted}
\end{examplebox}

\subsection{Lock Guards}
We can use \mintinline{Cpp}{scoped_lock}, \mintinline{Cpp}{unique_lock} or \mintinline{Cpp}{lock_guard} to link the time the lock is held to the lifetime of the lock guard object.
\begin{itemize}
	\item Each has slight differences, separate implementations are provided rather than using complex template magic.
	\item Deadlock avoidance is used to ensure all threads acquire and release locks in the same order.
\end{itemize}

\subsubsection{Scoped Lock}
\begin{minted}{Cpp}
  namespace std {
    template<class... MutexTypes>
    class scoped_lock {
    public:
      using mutex_type = Mutex;   // If MutexTypes... consists of the single type Mutex
    
      explicit scoped_lock(MutexTypes&... m);
      explicit scoped_lock(adopt_lock_t, MutexTypes&... m);
      ~scoped_lock();
    
      scoped_lock(const scoped_lock&) = delete;
      scoped_lock& operator=(const scoped_lock&) = delete;
    
    private:
      tuple<MutexTypes&...> pm;   // exposition only
    };
  }
\end{minted}
\begin{itemize}
	\item Constructed from one or more mutexes.
	\item Locks all mutexes on construction.
	\item Unlocks all mutexes on destruction.
\end{itemize}
\begin{consbox}
	Does not support deferred locking, early unlocking or ownership transfer (with \mintinline{Cpp}{std::move}).
\end{consbox}

\begin{minted}{Cpp}
#include <mutex>
#include <iostream>

std::mutex m1, m2;

static void some_fun() {
    std::scoped_lock lock(m1, m2); // acquire lock on m1 and m2 (or any number of locks)
    std::cout << "Critical region here" << std::endl;
}
\end{minted}

\subsubsection{Unique Lock}
\begin{minted}{Cpp}
namespace std {
  template<class Mutex>
  class unique_lock {
  public:
    using mutex_type = Mutex;
  
    // construct/copy/destroy
    unique_lock() noexcept;
    explicit unique_lock(mutex_type& m);

    // locking strategies
    unique_lock(mutex_type& m, defer_lock_t) noexcept;
    unique_lock(mutex_type& m, try_to_lock_t);
    unique_lock(mutex_type& m, adopt_lock_t);

    //
    template<class Clock, class Duration>
      unique_lock(mutex_type& m, const chrono::time_point<Clock, Duration>& abs_time);
    template<class Rep, class Period>
      unique_lock(mutex_type& m, const chrono::duration<Rep, Period>& rel_time);
    ~unique_lock();
  
    unique_lock(const unique_lock&) = delete;
    unique_lock& operator=(const unique_lock&) = delete;
  
    unique_lock(unique_lock&& u) noexcept;
    unique_lock& operator=(unique_lock&& u);
  
    // locking
    void lock();
    bool try_lock();
  
    template<class Rep, class Period>
      bool try_lock_for(const chrono::duration<Rep, Period>& rel_time);
    template<class Clock, class Duration>
      bool try_lock_until(const chrono::time_point<Clock, Duration>& abs_time);
  
    void unlock();
  
    // modifiers
    void swap(unique_lock& u) noexcept;
    mutex_type* release() noexcept;
  
    // observers
    bool owns_lock() const noexcept;
    explicit operator bool () const noexcept;
    mutex_type* mutex() const noexcept;
  
  private:
    mutex_type* pm;             // exposition only
    bool owns;                  // exposition only
  };
  
  template<class Mutex>
    void swap(unique_lock<Mutex>& x, unique_lock<Mutex>& y) noexcept;
}
\end{minted}
\begin{itemize}
	\item Constructed from one mutex.
	\item Locks mutex on construction by default, but can have locking deferred.
	\item Allows for unlocking and relocking.
	\item If mutex held on destruction, unlocks.
	\item Can transfer lock ownership with \mintinline{Cpp}{std::move}.
\end{itemize}
\begin{consbox}
	Only works for a single mutex.
\end{consbox}

\begin{examplebox}{Scoped out}
	Create a basic implementation of defer that could be used for a scoped lock.
	\tcblower
	\begin{minted}{Cpp}
#include <mutex>
#include <iostream>
#include <functional>

class Defer {
  private:
    std::function<void(void)> function_;

  public:
    Defer(std::function<void(void)> fun) : function_(fun) {}
    ~Defer() {
      function_();
    }
};


int main() {
  std::mutex m;

  Defer lock([&m] () {
    m.unlock();
    std::cout << "Unlocking" << std::endl;}
  );

  std::cout << "lets do some racey stuff here" << std::endl;
}
  \end{minted}
	We could also implement this pattern in rust. As mutexes already work this way in rust, we create a dummy mutex struct to use.
	\begin{minted}{Rust}
#![feature(fn_traits)]
struct Defer<F: FnMut()>(F);

impl<F: FnMut()> Drop for Defer<F> {
    fn drop(&mut self) {
        self.0()
    }
}

fn main() {
    let mut m = Mutex();
    m.lock();
    let _d = Defer(|| m.unlock());
    println!("lets do some racey stuff here")
}
  \end{minted}
\end{examplebox}

\section{Race Conditions in C++}
\begin{definitionbox}{Data Race}
	A data race is a race condition on the value of some data shared between threads.
	\begin{itemize}
		\item Distinct threads access a memory location.
		\item At least one thread modifies the location.
		\item At least of of the accesses is non-atomic (allows for operations of other threads to be interleaved)
		\item Accesses are not ordered by synchronisation (e.g for mutual exclusion)
	\end{itemize}
	Data races are value-oblivious, meaning a data race is present even if value of some shared data is not affected.
	\begin{itemize}
		\item e.g Two threads write the same value to the same place.
		\item e.g one thread stores the same value, another thread reads.
	\end{itemize}
	Always a bug, considered an unintentional race condition.
	\\
	\\ Synchronisation is achieved via:
	\begin{itemize}
		\item Mutexes
		\item Acquire load from release store
		\item sequentially consistent load from sequentially consistent store
	\end{itemize}
\end{definitionbox}
\begin{definitionbox}{Undefined Behaviour}
	\begin{quote}
		\textit{"Anything at all can happen; the Standard imposes no requirements. The program
			may fail to compile, or it may execute incorrectly (either crashing or silently
			generating incorrect results), or it may fortuitously do exactly what the programmer
			intended."} - \href{https://c-faq.com/ansi/undef.html}{C FAQ}
	\end{quote}
	\begin{itemize}
		\item Programmer must avoid relying on undefined behaviour.
		\item Different compilers implementing the specification can do anything with undefined behaviour.
		\item Allows compilers to do more optimisation (fewer guarantees to satisfy).
		\item Among the most cited language design issues with C++.
	\end{itemize}
	The behaviour of a C++ program on some input is undefined if a data race can occur. This means
	specification is saying a program with a data race can do \textit{anything}, there are no guarantees (even that the result depends on the outcome of the race).
\end{definitionbox}

Compilers typically optimise on the assumption there is no undefined behaviour.
\begin{itemize}
	\item If there is no undefined behaviour, then assuming none is fine.
	\item If there is undefined behaviour, the language specification says \textit{anything goes} and hence any output is valid.
\end{itemize}

\begin{minted}{Cpp}
#include <thread>

static void set_x(int& x) {
  x = 1;
}

static void wait_x(int& x) {
  while (x == 0);
}

int main() {
  int x = 0;
  std::thread t1(set_x, std::ref(x));
  std::thread t2(wait_x, std::ref(x));
  t1.join();
  t2.join();
}
\end{minted}
Here the loop in \mintinline{Cpp}{wait_x} can be optimised.
\\\begin{minipage}{.5\textwidth}
	\begin{minted}{Cpp}
static void wait_x(int& x) {
  int temp_register = r; 
  while (temp == 0);
}
  \end{minted}
\end{minipage}
\hfill
\begin{minipage}{.48\textwidth}
	\begin{enumerate}
		\item x is a non-atomic variable
		\item if another thread modified x, then there would be a data race.
		\item A data race is undefined behaviour
	\end{enumerate}
	Place copy of x into a register and compare using this.
\end{minipage}
\\\begin{minipage}{.5\textwidth}
	\begin{minted}{Cpp}
static void wait_x(int& x) {
  // terminate thread
}
  \end{minted}
\end{minipage}
\hfill
\begin{minipage}{.48\textwidth}
	\begin{enumerate}
		\item An infinite loop with no side effects is undefined behaviour.
		\item Can assume it is \textit{dead code} and remove.
	\end{enumerate}
	Dead code can be removed.
\end{minipage}

\subsection{Thread Sanitiser}
A sanitiser to automatically detect data races.
\begin{itemize}
	\item Available in clang++ and g++ compilers.
	\item Enabled with \mintinline{Bash}{-fsanitize=thread} and add debug symbols with \mintinline{Bash}{-g}.
\end{itemize}

\section{Condition Variables}
\begin{definitionbox}{Condition Variable}
	A condition that can be waited on, and notified.
	\begin{itemize}
		\item Threads can wait on the condition to be signalled.
		\item Can be used to construct a monitor.
		\item In languages without a monitor construct, an explicit lock is required.
	\end{itemize}
	\begin{minted}{Cpp}
#include <semaphore>
#include <mutex>
#include <deque>
#include <cassert> 

class condition_variable {
  public:
    // delete copy constructors
    condition_variable(const condition_variable&) = delete;
    condition_variable& operator=(const condition_variable&) = delete;

    void notify_all(std::unique_lock<std::mutex> monitor_lock&) {
      assert(monitor_lock.owns_lock())
      for (auto& sema : wait_semas_) {
        sema.release();
      }
      wait_semas_.clear();
    }

    void notify_one(std::unique_lock<std::mutex> monitor_lock&) {
      assert(monitor_lock.owns_lock())
      wait_semas_.pop_front().release();
    }

    void wait(std::unique_lock<std::mutex> monitor_lock&) {
      assert(monitor_lock.owns_lock())
      std::counting_semaphore wait_sema(0);
      wait_semas_.push_back(std::ref(wait_sema));
      monitor_lock.release();
      wait_sema.acquire();
      monitor_lock.aquire();
    }
  
  private:
    std::deque<std::ref<std::counting_semaphore>> wait_semas_;
};

}
  \end{minted}
\end{definitionbox}

\begin{minted}{Cpp}
#include <condition_variable>

namespace std {
  class condition_variable;
  class condition_variable_any;
  
  void notify_all_at_thread_exit(condition_variable& cond, unique_lock<mutex> lk);
  
  enum class cv_status { no_timeout, timeout };
}
\end{minted}

\subsection{Using Condition Variables}
The \mintinline{Cpp}{std::condition_variable} class is as follows:
\begin{minted}{Cpp}
namespace std {
  class condition_variable {
  public:
    condition_variable();
    ~condition_variable();
  
    // delete copy constructors
    condition_variable(const condition_variable&) = delete;
    condition_variable& operator=(const condition_variable&) = delete;

    // signal a condition variable
    void notify_one() noexcept;
    void notify_all() noexcept;

    // The current thread waits on the condition variable (until signalled), 
    // using the (acquired) lock to synchronise
    void wait(unique_lock<mutex>& lock);

    // Wait on a predict using the provided mutex using the (acquired) lock 
    // to synchronise
    template<class Pred>
      void wait(unique_lock<mutex>& lock, Pred pred);
    
    // wait until time
    template<class Clock, class Duration>
      cv_status wait_until(unique_lock<mutex>& lock,
                           const chrono::time_point<Clock, Duration>& abs_time);
    template<class Clock, class Duration, class Pred>
      bool wait_until(unique_lock<mutex>& lock,
                      const chrono::time_point<Clock, Duration>& abs_time, Pred pred);
    
    // wait for time
    template<class Rep, class Period>
      cv_status wait_for(unique_lock<mutex>& lock,
                         const chrono::duration<Rep, Period>& rel_time);
    template<class Rep, class Period, class Pred>
      bool wait_for(unique_lock<mutex>& lock,
                    const chrono::duration<Rep, Period>& rel_time, Pred pred);
 
    using native_handle_type = /* implementation-defined */;
    native_handle_type native_handle();
  };
}
\end{minted}
\begin{minipage}{.48\textwidth}
	\subsubsection{Wait on Signal}
	\begin{enumerate}
		\item Associated a \mintinline{Cpp}{std::mutex} with the condition variable.
		\item Acquire a lock on the mutex with a unique lock.
		\item Call wait with the lock.
	\end{enumerate}
	The thread will block until the condition variable is signalled.
	\begin{minted}{Cpp}
#include <mutex>
#include <condition_variable>

std::mutex m;
std::condition_variable cond;

static void some_func() {
    std::unique_lock<std::mutex> lock(m);
    cond.wait(lock);
}
  \end{minted}
\end{minipage}
\hfill
\vline
\hfill
\begin{minipage}{.48\textwidth}
	\subsubsection{Wait on predicate}
	\begin{enumerate}
		\item Associated a \mintinline{Cpp}{std::mutex} with the condition variable.
		\item Acquire a lock on the mutex with a unique lock.
		\item Call wait with a predicate.
	\end{enumerate}
	Immediately returns if the predicate is true. Otherwise blocks, when the condition variable is
	signalled, the predicate will be checked, if true the thread returns, otherwise the thread
	is blocked again.
	\begin{minted}{Cpp}
#include <mutex>
#include <condition_variable>

std::mutex m;
std::condition_variable cond;

static void some_func() {
    std::unique_lock<std::mutex> lock(m);
    cond.wait(lock, [...]() -> bool {...});
}
  \end{minted}
\end{minipage}


\section{Atomics}
Defined in the \mintinline{Cpp}{atomic} header. Allow for the construction of atomic variables for integral and arbitrary types (that are TriviallyCopyable, CopyConstructible and CopyAssignable).
\begin{itemize}
	\item For integral types atomic operations offer lower overhead alternatives for protecting small amounts of data than using a mutex.
	\item Atomic declarations prevent data races. This can be useful in declaring an intentional race condition (to prevent undefined behaviour)
\end{itemize}
\subsection{Atomic Template Class}
\begin{minted}{Cpp}
namespace std {
  template<class T> struct atomic {
    using value_type = T;
  
    static constexpr bool is_always_lock_free = /* implementation-defined */;
    bool is_lock_free() const volatile noexcept;
    bool is_lock_free() const noexcept;
  
    // operations on atomic types
    constexpr atomic() noexcept(is_nothrow_default_constructible_v<T>);
    constexpr atomic(T) noexcept;
    atomic(const atomic&) = delete;
    atomic& operator=(const atomic&) = delete;
    atomic& operator=(const atomic&) volatile = delete;
  
    // load the value (make a non-atomic copy of the current value)
    T load(memory_order = memory_order::seq_cst) const volatile noexcept;
    T load(memory_order = memory_order::seq_cst) const noexcept;

    // implicit conversion from an instance of this class (std::atomic<T>) to T (used by static_cast)
    operator T() const volatile noexcept;
    operator T() const noexcept;

    // Store (overwrite) the the atomic
    void store(T, memory_order = memory_order::seq_cst) volatile noexcept;
    void store(T, memory_order = memory_order::seq_cst) noexcept;

    // Assignment 
    T operator=(T) volatile noexcept;
    T operator=(T) noexcept;
    
    // Store desired and return the old value atomically
    T exchange(T desired, memory_order = memory_order::seq_cst) volatile noexcept;
    T exchange(T desired, memory_order = memory_order::seq_cst) noexcept;

    // if the old value is expected, then replace with desired and return true.
    // if the old value is not expected, then return false
    // the old value (regardless is expected) is placed in expected
    bool compare_exchange_strong(T& expected, T desired, memory_order, memory_order) volatile noexcept;
    bool compare_exchange_strong(T& expected, T desired, memory_order, memory_order) noexcept;
    bool compare_exchange_strong(T& expected, T desired,
                                  memory_order = memory_order::seq_cst) volatile noexcept;
    bool compare_exchange_strong(T& expected, T desired, memory_order = memory_order::seq_cst) noexcept;

    // Same as compare_exchange_strong on x86, but different on ARM. Can fail spuriously.
    bool compare_exchange_weak(T& expected, T desired, memory_order, memory_order) volatile noexcept;
    bool compare_exchange_weak(T& expected, T desired, memory_order, memory_order) noexcept;
    bool compare_exchange_weak(T& expected, T desired,
                                memory_order = memory_order::seq_cst) volatile noexcept;
    bool compare_exchange_weak(T& expected, T desired, memory_order = memory_order::seq_cst) noexcept;
  
    // check value of this against old, if equal => blocks until notify called.
    void wait(T old, memory_order = memory_order::seq_cst) const volatile noexcept;
    void wait(T old, memory_order = memory_order::seq_cst) const noexcept;

    void notify_one() volatile noexcept;
    void notify_one() noexcept;
    void notify_all() volatile noexcept;
    void notify_all() noexcept;
  };
}
\end{minted}
Hence we can use it to make access to complex objects atomic. Large types (i.e not integral types) use locks for this.
\begin{minted}{Cpp}
#include <atomic>

// an atomically accessed struct
typedef struct {
  int a;
  bool c;
} MyStruct;

std::atomic<MyStruct> p({.a=1, .c=true});
\end{minted}
\begin{sidenotebox}{Weak vs Strong Exchange on ARM}
	The arm architecture allows exchange to spuriously fail.
	\begin{itemize}
		\item This is documented behaviour for \mintinline{Cpp}{exchange_weak}
		\item \mintinline{Cpp}{exchange_strong} contains a loop that makes use of \mintinline{Cpp}{exchange_weak}
		\item On x86 strong and weak are identical and do not spuriously fail.
	\end{itemize}
\end{sidenotebox}
\subsection{Atomic Integral Types}
For integral types, fast assembly supported instructions for atomic integers, booleans and floats can be used.

\begin{minted}{Cpp}
  namespace std {
    template<> struct atomic</* integral */> {
      // ... normal operations from std::atomic<T>

      // Atomic operations
      /* integral */ fetch_add(/* integral */, memory_order = memory_order::seq_cst) volatile noexcept;
      /* integral */ fetch_add(/* integral */, memory_order = memory_order::seq_cst) noexcept;
      /* integral */ fetch_sub(/* integral */, memory_order = memory_order::seq_cst) volatile noexcept;
      /* integral */ fetch_sub(/* integral */, memory_order = memory_order::seq_cst) noexcept;
      /* integral */ fetch_and(/* integral */, memory_order = memory_order::seq_cst) volatile noexcept;
      /* integral */ fetch_and(/* integral */, memory_order = memory_order::seq_cst) noexcept;
      /* integral */ fetch_or(/* integral */, memory_order = memory_order::seq_cst) volatile noexcept;
      /* integral */ fetch_or(/* integral */, memory_order = memory_order::seq_cst) noexcept;
      /* integral */ fetch_xor(/* integral */, memory_order = memory_order::seq_cst) volatile noexcept;
      /* integral */ fetch_xor(/* integral */, memory_order = memory_order::seq_cst) noexcept;

      // Operator Overloads
      /* integral */ operator++(int) volatile noexcept;
      /* integral */ operator++(int) noexcept;
      /* integral */ operator--(int) volatile noexcept;
      /* integral */ operator--(int) noexcept;
      /* integral */ operator++() volatile noexcept;
      /* integral */ operator++() noexcept;
      /* integral */ operator--() volatile noexcept;
      /* integral */ operator--() noexcept;
      /* integral */ operator+=(/* integral */) volatile noexcept;
      /* integral */ operator+=(/* integral */) noexcept;
      /* integral */ operator-=(/* integral */) volatile noexcept;
      /* integral */ operator-=(/* integral */) noexcept;
      /* integral */ operator&=(/* integral */) volatile noexcept;
      /* integral */ operator&=(/* integral */) noexcept;
      /* integral */ operator|=(/* integral */) volatile noexcept;
      /* integral */ operator|=(/* integral */) noexcept;
      /* integral */ operator^=(/* integral */) volatile noexcept;
      /* integral */ operator^=(/* integral */) noexcept;

      // ... normal operations from std::atomic<T>
    };
  }
\end{minted}
For example we can see a single add is used for the \mintinline{Cpp}{fetch_add} here.
\\ \begin{minipage}{.49\textwidth}
	\begin{minted}[bgcolor=threadYellow]{Cpp}
#include <atomic>

int main() {
    std::atomic<int> x(1);
    x += 3;
}
  \end{minted}
\end{minipage}
\hfill
\begin{minipage}{.49\textwidth}
	\begin{minted}[bgcolor=threadBlue]{NASM}
main:
  mov       DWORD PTR [rsp-4], 1
  lock add  DWORD PTR [rsp-4], 3
  xor       eax, eax
  ret

  \end{minted}
\end{minipage}

Operator overloading is available for the integral types, however it can be difficult to determine which operations are atomic.
\begin{minted}{Cpp}
  x = 42;        /* equivalent to */  x.store(42);
  y = x;         /* equivalent to */  y = x.load();
  x++;           /* equivalent to */  x.fetch_add(1);
  y = ++x;       /* equivalent to */  y = x.fetch_add(1) + 1;
  x += 42;       /* equivalent to */  x.fetch_add(42);
  y = (x += 42); /* equivalent to */  y = x.fetch_add(42) + 42;
\end{minted}

\section{Memory Order}
The \mintinline{Cpp}{atomic} header provides several memory orderings:
\begin{minted}{Cpp}
namespace std {
  // ...

  enum class memory_order : /* unspecified */ {
    relaxed, consume, acquire, release, acq_rel, seq_cst
  };
  inline constexpr memory_order memory_order_relaxed = memory_order::relaxed;
  inline constexpr memory_order memory_order_consume = memory_order::consume;
  inline constexpr memory_order memory_order_acquire = memory_order::acquire;
  inline constexpr memory_order memory_order_release = memory_order::release;
  inline constexpr memory_order memory_order_acq_rel = memory_order::acq_rel;
  inline constexpr memory_order memory_order_seq_cst = memory_order::seq_cst;

  //...
}
\end{minted}

\begin{definitionbox}{Sequential Consistency}
	The order of operations are executed as in the order of the program text.
	\begin{itemize}
		\item The default memory ordering for atomics (\mintinline{Cpp}{std::atomics::memory_order_seq_cst})
	\end{itemize}
	\begin{tabbox}{prosbox}
		\textbf{Simple} & Easy to reason about the interleaving of threads. \\
	\end{tabbox}
	\begin{tabbox}{consbox}
		\textbf{Expensive} & Compiler uses memory barriers which restrict how instructions can be reordered and optimised. \\
	\end{tabbox}
\end{definitionbox}

\begin{definitionbox}{Relaxed Memory Order}
	Guarantees only sequential consistency per location.
\end{definitionbox}

\section{Message Passing}
Threads can communicate without blocking through atomics.
\begin{itemize}
	\item Poll on an atomic variable (potentially doing some other work while waiting)
	\item Often used for synchronising access to shared resources (e.g spin locks)
\end{itemize}

\subsection{Expensive Approach}
We can use sequential consistency to ensure that the
\begin{minted}[bgcolor=threadGrey]{Cpp}  
#include <atomic>

std::atomic<bool> flag(true);
SharedObj data(some_data);
\end{minted}
\begin{minipage}{.49\textwidth}
	\begin{minted}[bgcolor=threadYellow]{Cpp}
use_data(data);

flag.store(true);


  \end{minted}
\end{minipage}
\hfill
\begin{minipage}{.49\textwidth}
	\begin{minted}[bgcolor=threadBlue]{Cpp}
while (!flag.load()) {
// do nothing - a pure spinlock
}

use_data(data);
  \end{minted}
\end{minipage}

\begin{tabbox}{consbox}
	\textbf{Slow} &  On some architectures memory barriers are required for to ensure sequential consistency, are expensive. \\
\end{tabbox}

\begin{definitionbox}{Memory Barrier / Fence}
	These prevent the reordering of load and store instructions by dynamically scheduled processors.
	\begin{itemize}
		\item On a single core processor this is not an issue (dynamic scheduling commits instructions effects in order)
		\item On a multicore system instruction reordering in execution stages can result in non-sequentially consistent accesses.
		\item Dynamic scheduling of instructions improves performance by filling potential \textit{stalls} with useful computation.
	\end{itemize}
\end{definitionbox}

\subsection{Incorrect Approach}
One approach could be to use relaxed memory ordering.
\begin{itemize}
	\item Allows for other values (e.g the data being protected by a spinlock) to be re-ordered around the atomic.
	\item This removes the protection the spinlock is intended to provide.
\end{itemize}
\noindent
\begin{minipage}{.49\textwidth}
	\begin{minted}[bgcolor=threadYellow]{Cpp}
// These can be reordered
use_data(data);

flag.store(true, std::memory_order_relaxed);
  
    \end{minted}
\end{minipage}
\hfill
\begin{minipage}{.49\textwidth}
	\begin{minted}[bgcolor=threadBlue]{Cpp}
while (!flag.load(std::memory_order_relaxed)) {
// do nothing - a pure spinlock
}

    use_data(data);
    \end{minted}
\end{minipage}

\subsection{Release-Acquire Consistency}
Release acquire consistency
\\ \begin{minipage}{.49\textwidth}
	\begin{minted}[bgcolor=threadYellow]{Cpp}
// These can be reordered
use_data(data);

flag.store(true, std::memory_order_release);

  \end{minted}
\end{minipage}
\hfill
\begin{minipage}{.49\textwidth}
	\begin{minted}[bgcolor=threadBlue]{Cpp}
while (!flag.load(std::memory_order_acquire)) {
// do nothing - a pure spinlock
}

  use_data(data);
  \end{minted}
\end{minipage}
The load is hence synchronised with the store, and reordering is prevented.
\begin{itemize}
	\item Ensures correctness
	\item Potentially uses fewer memory barriers than SC (depends on the architecture)
\end{itemize}
